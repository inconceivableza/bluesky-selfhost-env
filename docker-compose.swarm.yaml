version: '3.8'

configs:
    certs-ca:
      file: ./certs/ca-certificates.crt
    certs-intermediate:
      file: ./certs/intermediate.crt
    certs-root:
      file: ./certs/root.crt

x-certs-config: &certs-config
  - source: certs-ca
    target: ${CUSTOM_CERTS_DIR-/tmp/dummy-certs}/ca-certificates.crt
  - source: certs-intermediate
    target: ${CUSTOM_CERTS_DIR-/tmp/dummy-certs}/intermediate.crt
  - source: certs-root
    target: ${CUSTOM_CERTS_DIR-/tmp/dummy-certs}/root.crt

# These must be created before deploying
secrets:
  db_secrets:
    external: true
  bgs_secrets:
    external: true
  pds_secrets:
    external: true
  bsky_secrets:
    external: true
  plc_secrets:
    external: true
  ozone_secrets:
    external: true
  opensearch_secrets:
    external: true
  palomar_secrets:
    external: true
  social_link_secrets:
    external: true
  backup_secrets:
    external: true
  google_backup_credentials:
    external: true
  
volumes:
  caddy-data:
  caddy-config:
  database:
  opensearch:
  bgs:
  bsky:
  feed-generator:
  pds:
  redis:
  jetstream:
  backup-staging:
  backup-repo:
  ipcc:
  opensearch-dashboards:
  fluent-bit-storage:

networks:
  foodios-net:
    driver: overlay
    attachable: true

x-deploy-defaults: &deploy-defaults
  replicas: 1
  update_config:
    parallelism: 1
    delay: 10s
    order: start-first
    monitor: 60s
  restart_policy:
    condition: on-failure
    delay: 5s
    max_attempts: 3
    window: 60s
  placement:
    constraints:
      - node.role == manager

services:
  caddy:
    #   reverse proxy
    #   cf. https://blog.kurokobo.com/archives/3669#Caddy_acme_server
    image: ${BRANDED_NAMESPACE}/caddy:${CADDY_IMAGE_TAG:-2}
    build:
      context: .
      dockerfile: ops/caddy-Dockerfile
      args:
        CADDY_DNS_PACKAGE_SRC: ${CADDY_DNS_PACKAGE_SRC-github.com/caddy-dns/cloudflare}
    ports:
      # these ports can be overridden in .env if using haproxy to forward requests to caddy
      - target: 80
        published: ${CADDY_HTTP_PORT:-80}
        mode: host
      - target: 443
        published: ${CADDY_HTTPS_PORT:-443}
        mode: host
      - target: 443
        published: ${CADDY_HTTPS_PORT:-443}
        protocol: udp
        mode: host
      # - 9000:9000 # internal CA server; doesn't need to be public
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - DOMAIN=${DOMAIN}
      - EMAIL4CERTS=${EMAIL4CERTS}
      - apiFQDN=${apiFQDN}
      - bgsFQDN=${bgsFQDN}
      - bskyFQDN=${bskyFQDN}
      - cardFQDN=${cardFQDN}
      - embedFQDN=${embedFQDN}
      - feedgenFQDN=${feedgenFQDN}
      - ipFQDN=${ipFQDN}
      - jetstreamFQDN=${jetstreamFQDN}
      - linkFQDN=${linkFQDN}
      - logsFQDN=${logsFQDN}
      - ozoneFQDN=${ozoneFQDN}
      - palomarFQDN=${palomarFQDN}
      - pdsFQDN=${pdsFQDN}
      - plcFQDN=${plcFQDN}
      - publicApiFQDN=${publicApiFQDN}
      - socialappFQDN=${socialappFQDN}
      - haproxyFORWARD=${haproxyFORWARD-}
      - TRUSTED_PROXIES=${TRUSTED_PROXIES-}
      - CADDY_DNS_PROVIDER=${CADDY_DNS_PROVIDER-}
      - CADDY_DNS_API_TOKEN=${CADDY_DNS_API_TOKEN-}
      - CADDY_DNS_RESOLVER=${CADDY_DNS_RESOLVER-}
      - CADDY_DNS_USED=${CADDY_DNS_USED-}
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=caddy
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./config/caddy-dynamic.env:/etc/caddy-dynamic.env:ro
      # CA certificates for serving self-signed. >>>
      - ./certs/root.crt:${CADDY_CERTS_DIR-/tmp/dummy-certs}/root.crt:ro
      - ./certs/root.key:${CADDY_CERTS_DIR-/tmp/dummy-certs}/root.key:ro
      # CA certificates for serving self-signed. <<<
      - caddy-data:/data
      - caddy-config:/config
      # the social app's static website. TODO: uncomment when there's something in here to copy
      # - ./${PDS_WEB_DIR:-staticweb}:/socialweb:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:2019/metrics"]
      interval: 5s
      timeout: 3s
      retries: 20
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  caddy-sidecar:
    # decides which domains caddy should generate certificates for
    image: httpd:2
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    configs: *certs-config
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  database:
    image: postgres:16-trixie
    command: ['sh', '-c', 'set -a && . /run/secrets/db_secrets && set +a && exec docker-entrypoint.sh postgres']
    ports:
      - target: 5432
        published: 5432
        mode: host
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - POSTGRES_INITDB_ARGS=-A scram-sha-256 --encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_DB=healthcheck
    secrets:
      - db_secrets
    volumes:
      - ./config/init-postgres:/docker-entrypoint-initdb.d/:ro
      - database:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U pg -d healthcheck >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
      placement:
        constraints:
          - node.role == manager
          # Add label constraint if you want to pin to specific node:
          # - node.labels.postgres == true
    networks:
      - foodios-net

  pgadmin:
    image: dpage/pgadmin4
    command: ['sh', 'c', 'sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; ./entrypoint.sh']
    ports:
      - target: 80
        published: 54321
        mode: host
    environment:
      # TODO store in secret
      - PGADMIN_DEFAULT_EMAIL=example@example.com
      - PGADMIN_DEFAULT_PASSWORD=password
    depends_on:
      - database
    deploy:
      <<: *deploy-defaults
      replicas: 1  # Set to 1 to enable pgadmin
    networks:
      - foodios-net
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh

  redis:
    image: redis:8-bookworm
    volumes:
      - redis:/data/
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  opensearch:
    image: ${BRANDED_NAMESPACE}/bluesky-indigo-opensearch:${branded_asof:-latest}
    # profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/palomar/Dockerfile.opensearch
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
         # cf. https://github.com/opensearch-project/OpenSearch/issues/8215
       - OPENSEARCH_JAVA_OPTS=${OPENSEARCH_JAVA_OPTS-}
    command: ['sh', '-c', 'set -a; . /run/secrets/opensearch_secrets; set +a;  exec /usr/share/opensearch/opensearch-docker-entrypoint.sh']
    ports:
      - target: 9200
        published: 9200
        mode: host
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - bootstrap.memory_lock=true
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - node.name=os-node
      - plugins.security.disabled=true
      - transport.host=127.0.0.1
    secrets:
      - opensearch_secrets
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - opensearch:/usr/share/opensearch/data/
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:9200/_cluster/health | grep '\"status\":' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    networks:
      - foodios-net

  plc:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-did-method-plc:${PLC_IMAGE_TAG:-latest}
    # profiles: ['', 'did-method-plc']
    build:
      context: ./repos/did-method-plc/
      dockerfile: packages/server/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/did-method-plc"
        org.opencontainers.image.description: "${REBRANDING_NAME} DID PLC server"
    user: root
    command: ['sh', '-c', 'set -a; . /run/secrets/plc_secrets; set +a; ${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; su node -c "dumb-init -- node --enable-source-maps index.js"']      
    ports:
      - target: 2582
        published: 2582
        mode: host
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PORT=2582
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_PROPAGATION_STYLE=tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
    secrets:
      - plc_secrets
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:2582/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - database
      - caddy
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    configs: *certs-config

  pds:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-atproto-pds:${PDS_IMAGE_TAG:-latest}
    command: ['sh', '-c', 'set -a; . /run/secrets/pds_secrets; set +a; ${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; exec node --heapsnapshot-signal=SIGUSR2 --enable-source-maps --require=./tracer.js index.js']
    # profiles: ['', 'atproto']
    build:
      context: ./repos/social-app/submodules/atproto/
      dockerfile: services/pds/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/atproto"
        org.opencontainers.image.description: "${REBRANDING_NAME} ATP Personal Data Server (PDS)"
    ports:
      - target: 2583
        published: 2583
        mode: host
    # expose:
    #   - 2583
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root
    user: root
    environment:
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PDS_BLOBSTORE_DISK_LOCATION=/pds/blobs
      - PDS_BSKY_APP_VIEW_DID=did:web:${bskyFQDN}
      - PDS_BSKY_APP_VIEW_URL=https://${bskyFQDN}
      - PDS_CRAWLERS=https://${bgsFQDN}
      - PDS_DATA_DIRECTORY=/pds
      - PDS_DEV_MODE=true
#     - PDS_BLOBSTORE_DISK_TMP_LOCATION=/pds/image/tmp
      - PDS_DID_PLC_URL=https://${plcFQDN}
      - PDS_EMAIL_FROM_ADDRESS=${PDS_EMAIL_FROM_ADDRESS}
      - PDS_EMAIL_IMAGES_BASE_URL=${PDS_EMAIL_IMAGES_BASE_URL}
      - PDS_EMAIL_SMTP_URL=${PDS_EMAIL_SMTP_URL}
      - PDS_ENABLE_DID_DOC_WITH_SESSION=true
      - PDS_HOSTNAME=${pdsFQDN}
      - PDS_INVITE_INTERVAL=${PDS_INVITE_INTERVAL-}
      - PDS_INVITE_REQUIRED=${PDS_INVITE_REQUIRED}
      - PDS_PORT=2583
# starts: unset optional env for testing ozone >>>>>>>
#     - PDS_MOD_SERVICE_DID=did:web:${ozoneFQDN}
#     - PDS_MOD_SERVICE_URL=https://${ozoneFQDN}
#     - PDS_REPORT_SERVICE_DID=did:web:${ozoneFQDN}
#     - PDS_REPORT_SERVICE_URL=https://${ozoneFQDN}
# ends: unset optional env for tesing ozone <<<<<<<
      - PDS_SERVICE_DID=did:web:${pdsFQDN}
      - PDS_SOCIAL_APP_DESCRIPTION=${SOCIAL_APP_DESCRIPTION}
      - PDS_SOCIAL_APP_EMOJI=${SOCIAL_APP_EMOJI}
      - PDS_SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - PDS_SOCIAL_APP_URL=${SOCIAL_APP_URL}
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_OTEL_ENABLED=true
      # the package.json misconfigures this service as plc-service
      - DD_SERVICE=pds
      - DD_TRACE_PROPAGATION_STYLE=datadog,tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
      - OTEL_PROPAGATORS=datadog,tracecontext,baggage
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    secrets:
      - pds_secrets
    volumes:
      - pds:/pds
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:2583/xrpc/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
      resources:
        limits:
          memory: 2G
    networks:
      - foodios-net
    depends_on:
      - database
      - caddy

  bgs:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-bgs:${BGS_IMAGE_TAG:-latest}
    # profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/bigsky/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/indigo"
        org.opencontainers.image.description: "${REBRANDING_NAME} ATP Relay (aka BGS)"
    ports:
      - target: 2470
        published: 2470
        mode: host
      - target: 2471
        published: 2471
        mode: host
    command: ["sh", "-c", 'set -a; . /run/secrets/bgs_secrets; set +a; ${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432;  exec /bigsky --db-tracing']
    environment:
      - ATP_PLC_HOST=https://${plcFQDN}
      - DATA_DIR=/data/bigsky
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - RELAY_DEFAULT_REPO_LIMIT=100000
      - RELAY_NEWPDS_PERDAY_LIMIT=500
      - RELAY_PERSISTER_DIR=/data/bigsky/events
    secrets:
      - bgs_secrets
    volumes:
      - bgs:/data/bigsky
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:2470/xrpc/_health | grep 'status.*ok' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
      resources:
        limits:
          memory: 2G
    networks:
      - foodios-net
    depends_on:
      - database
      - caddy

  bsky:
    # appview(api)
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-atproto-bsky:${BSKY_IMAGE_TAG:-latest}
    # profiles: ['', 'atproto']
    build:
      context: ./repos/social-app/submodules/atproto/
      dockerfile: services/bsky/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/atproto"
        org.opencontainers.image.description: "${REBRANDING_NAME} App View"
    ports:
      - target: 2584
        published: 2584
        mode: host
      # dataplane port # temporarily exposing for debugging
      - target: 3001
        published: 3001
        mode: host
      # bsync port # temporarily exposing for debugging
      - target: 3002
        published: 3002
        mode: host
    # expose:
    #   - 2584
    command: ["sh", "-c", 'set -a; . /run/secrets/bsky_secrets; set +a; ${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432;  exec node --enable-source-maps api.js']
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root
    user: root
    environment:
      - BSKY_BLOB_CACHE_LOC=/cache/
      - BSKY_BSYNC_HTTP_VERSION=1.1
      - BSKY_BSYNC_PORT=3002
      - BSKY_BSYNC_URL=http://bsky:3002
      - BSKY_COURIER_API_KEY=
      - BSKY_COURIER_HTTP_VERSION=
      - BSKY_COURIER_IGNORE_BAD_TLS=
      - BSKY_COURIER_URL=
      - BSKY_DATAPLANE_HTTP_VERSION=1.1
      - BSKY_DATAPLANE_PORT=3001
      - BSKY_DATAPLANE_URLS=http://bsky:3001
      - BSKY_DB_POSTGRES_SCHEMA=bsky
      - BSKY_DID_PLC_URL=https://${plcFQDN}
      - BSKY_LABELS_FROM_ISSUER_DIDS=${BSKY_LABELS_FROM_ISSUER_DIDS}
      - BSKY_PORT=2584
      - BSKY_PUBLIC_URL=https://${bskyFQDN}
      - BSKY_REPO_PROVIDER=wss://${bgsFQDN}
      - BSKY_SEARCH_URL=https://${palomarFQDN}
      - BSKY_SERVER_DID=did:web:${bskyFQDN}
      - BSKY_STATSIG_ENV=${BSKY_STATSIG_ENV-}
      - DEBUG_MODE=1
      - DID_PLC_URL=https://${plcFQDN}
      - ENABLE_MIGRATIONS=false
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - MOD_SERVICE_DID=did:web:${ozoneFQDN}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - REDIS_HOST=redis
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_PROPAGATION_STYLE=tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
    secrets:
      - bsky_secrets
    volumes:
      - bsky:/cache/
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:2584/ | grep 'AppView' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
      resources:
        limits:
          memory: 2G
    networks:
      - foodios-net
    depends_on:
      - database
      - redis
      - caddy

  social-app:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-app:${SOCIAL_APP_IMAGE_TAG:-latest}
    # the social-app Dockerfile compiles for this platform
    # platform: linux/amd64
    # profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
       - SENTRY_ORG=${SENTRY_ORG-}
       - SENTRY_PROJECT=${SENTRY_PROJECT-}
       - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT-}
       - EXPO_PUBLIC_SENTRY_DSN=${EXPO_PUBLIC_SENTRY_DSN-}
       - EXPO_PUBLIC_STATSIG_CLIENT_KEY=${EXPO_PUBLIC_STATSIG_CLIENT_KEY-}
       - EXPO_PUBLIC_STATSIG_API_URL=${EXPO_PUBLIC_STATSIG_API_URL-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} Web App"
    ports:
      - target: 8100
        published: 8100
        mode: host
    command: ["/bin/sh", "-c", "${UPDATE_CERTS_CMD:-true}; /usr/bin/bskyweb serve"]
    environment:
      # cf. https://github.com/bluesky-social/social-app/blob/main/bskyweb/example.env and https://github.com/bluesky-social/bsky-docs/issues/63 :
      - EXPO_PUBLIC_ATP_APPVIEW_DID=did:web:${apiFQDN}
      - EXPO_PUBLIC_ATP_APPVIEW_URL=https://${apiFQDN}
      - ATP_APPVIEW_URL=https://${apiFQDN}
      - EXPO_PUBLIC_ATP_PDS_DID=did:web:${pdsFQDN}
      - EXPO_PUBLIC_ATP_PDS_URL=https://${pdsFQDN}
      - EXPO_PUBLIC_ATP_PUBLIC_APPVIEW_URL=https://${publicApiFQDN}
      - EXPO_PUBLIC_BLUESKY_PROXY_DID=did:web:${bskyFQDN}
      - EXPO_PUBLIC_CHAT_PROXY_DID=${dmServiceDID}
      # - BSKY_CANONICAL_INSTANCE=true # Enable if this is the canonical deployment. This enables routes like /ips-v4, /ips-v6, /security.txt and /.well-known/*
      # - BRANDING_FILE=branding.json # This will usually use the branding.json generated by step10 by exporting from branding.yml
      - EXPO_PUBLIC_CORS_ALLOWED_ORIGINS=https://${socialappFQDN}
      - CORS_ALLOWED_ORIGINS=https://${socialappFQDN}
      - EXPO_PUBLIC_GIF_HOST=${gifFQDN}
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - HTTP_ADDRESS=:8100
      - EXPO_PUBLIC_GEOLOCATION_CONFIG_URL=https://${ipFQDN}/config
      - IPCC_HOST=http://ipcc:8080
      - EXPO_PUBLIC_LINK_HOST=${linkFQDN}
      - LINK_HOST=${linkFQDN}
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - EXPO_PUBLIC_OGCARD_URL=https://${cardFQDN}
      - OGCARD_URL=https://${cardFQDN}
      # comma-separated list for index.html
      - PRECONNECT_DOMAINS=https://${pdsFQDN},https://${linkFQDN}
      # we don't yet have one of these
      - EXPO_PUBLIC_PREVIEW_LINK_META_PROXY=
      # - ROBOTS_DISALLOW_ALL=true # enable this flag to disallow crawling
      - SENTRY_AUTH_TOKEN=${SENTRY_AUTH_TOKEN-}
      # - EXPO_PUBLIC_STATIC_CDN_HOST= # optional variable that is used to serve up fonts and icons
      - EXPO_PUBLIC_SOCIAL_APP_ABOUT=https://${pdsFQDN}
      - EXPO_PUBLIC_SOCIAL_APP_HOST=${socialappFQDN}
      - EXPO_PUBLIC_SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - EXPO_PUBLIC_SOCIAL_APP_URL=https://${socialappFQDN}
      - EXPO_PUBLIC_SOCIAL_EMBED_SERVICE=https://${embedFQDN}
      - EXPO_PUBLIC_SOCIAL_HELP_DESK_URL=${SOCIAL_HELP_DESK_URL}
      - EXPO_PUBLIC_SOCIAL_POLICY_BASE_URL=${SOCIAL_POLICY_BASE_URL}
      - EXPO_PUBLIC_STATSIG_CLIENT_KEY=${STATSIG_CLIENT_KEY}
      - EXPO_PUBLIC_STATSIG_API_URL=${STATSIG_API_URL}
      - EXPO_PUBLIC_STATUS_PAGE_URL=${STATUS_PAGE_URL}
      # we don't yet have one of these
      - EXPO_PUBLIC_VIDEO_SERVICE=
      - EXPO_PUBLIC_VIDEO_SERVICE_DID=
      - SOCIAL_APP_SECURITY_EMAIL=${SOCIAL_APP_SECURITY_EMAIL}
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:8100 | grep 'API docs at https://atproto.com' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  social-card:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-card:${SOCIAL_CARD_IMAGE_TAG:-latest}
    # the social-app Dockerfile compiles for this platform
    # platform: linux/amd64
    # profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.bskyogcard
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - SOCIAL_APP_NAME=${SOCIAL_APP_NAME-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} Card Service"
    ports: []
      # - 3003:3003
    user: root
    command: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432;  su node -c "dumb-init -- node --heapsnapshot-signal=SIGUSR2 --enable-source-maps dist/bin.js"']
    environment:
      - CARD_PORT=3003
      - CARD_APPVIEW_URL=https://${apiFQDN}
      # Checks a req header x-origin-verify and returns 404 if not matching
      - CARD_ORIGIN_VERIFY=
      - SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3003/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  social-embed:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-embed:${SOCIAL_EMBED_IMAGE_TAG:-latest}
    # the social-app Dockerfile compiles for this platform
    # platform: linux/amd64
    # profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.embedr
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} embed web app"
    ports: []
      # - 8101:8101 # listening port
      # - 9090:9090 # metrics port
    command: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; apt-get install -y wget ; dumb-init -- /usr/bin/embedr serve']
    environment:
      - ATP_PUBLIC_APPVIEW_URL=https://${publicApiFQDN}
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - HTTP_ADDRESS=:8101
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      # in dev mode, bskyembed reads from its .env.* VITE_ variables which are interpreted into html
      # however, we're actually not using bskyembed/ (ts) but bskyweb/cmd/embedr (go), which doesn't use these
      # we rather use a combination of .env.go-templates which puts "{{ .varname }}" syntax into the templates, for server-side rendering
      # and a /env-config.json URL which is served up using the environment or command-line parameters
      - OGCARD_URL=https://${cardFQDN}
      - SOCIAL_EMBED_SERVICE=https://${embedFQDN}
      - LINK_URL=https://${linkFQDN}
      - SOCIAL_APP_ABOUT=https://${pdsFQDN}/about
      - SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - SOCIAL_APP_URL=https://${socialappFQDN}
      # - SOCIAL_APP_SECURITY_EMAIL=
      # - SOCIAL_APP_SUPPORT_EMAIL=
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8101/ | grep 'Embed' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  social-link:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME}-social-link:${SOCIAL_LINK_IMAGE_TAG:-latest}
    # the social-app Dockerfile compiles for this platform
    # platform: linux/amd64
    # profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.bskylink
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} Link Service"
    ports: []
      # - 3004:3004 # listening port
    user: root
    command: ['sh', '-c', 'set -a; . /run/secrets/social_link_secrets; set +a; ${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; exec su node -c "dumb-init -- node --heapsnapshot-signal=SIGUSR2 --enable-source-maps dist/bin.js"']
    environment:
      - LINK_PORT=3004
      - LINK_HOSTNAMES=${linkFQDN}
      - LINK_APP_HOSTNAME=${socialappFQDN}
      # - LINK_DB_POSTGRES_MIGRATION_URL= # only set if you want to migrate from a previous database
      - LINK_DB_POSTGRES_SCHEMA=
      # just use the defaults for the pool
      # - LINK_DB_POSTGRES_POOL_SIZE=
      # - LINK_DB_POSTGRES_POOL_MAX_USES=
      # - LINK_DB_POSTGRES_POOL_IDLE_TIMEOUT_MS=
      - LINK_SAFELINK_ENABLED=0
      - LINK_SAFELINK_PDS_URL=https://${pdsFQDN}
      - LINK_SAFELINK_AGENT_IDENTIFIER=
      - LINK_SAFELINK_AGENT_PASS=
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    secrets:
      - social_link_secrets
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3004/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - database
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh

  palomar:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-palomar:${PALOMAR_IMAGE_TAG:-latest}
    # profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/palomar/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} atproto Search Service"
    command: ['sh', '-c', 'set -a; . /run/secrets/palomar_secrets; set +a; ${UPDATE_CERTS_CMD:-true}; sh /usr/local/bin/wait-for-ports.sh -t 120 opensearch:9200 database:5432; exec /palomar run']
    ports:
      - target: 3999
        published: 3999
        mode: host
      # port 3998 is the default metrics port
    environment:
      # refer https://github.com/bluesky-social/indigo/tree/main/cmd/palomar
      - ATP_BGS_HOST=wss://${bgsFQDN}
      - ATP_PLC_HOST=https://${plcFQDN}
      # - ES_CERT_FILE=
      - ES_HOSTS=http://opensearch:9200
      - ES_INSECURE_SSL=true
      - ES_USERNAME=admin
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PALOMAR_BIND=0.0.0.0:3999
      - PALOMAR_DISCOVER_REPOS=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    secrets:
      - palomar_secrets
    configs: *certs-config
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3999/_health | grep '\"status\":\"ok\"' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - opensearch
      - database
      - caddy
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh

  jetstream:
    image: ${BRANDED_NAMESPACE}/bluesky-jetstream:${branded_asof:-latest}
    # profiles: ['', 'jetstream']
    build:
      context: ./repos/jetstream/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - target: 6008
        published: 6008
        mode: host
    volumes:
      - jetstream:/data
    environment:
      - JETSTREAM_WS_URL=wss://${bgsFQDN}/xrpc/com.atproto.sync.subscribeRepos
      - JETSTREAM_DATA_DIR=/data
      - JETSTREAM_LISTEN_ADDR=:6008
      - JETSTREAM_METRICS_LISTEN_ADDR=:6009
      - JETSTREAM_LIVENESS_TTL=96h
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - caddy

  ozone-standalone:
    # moderation-api:
    image: ${BRANDED_NAMESPACE}/bluesky-ozone:${branded_asof:-latest}
    # profiles: ['', 'ozone']
    build:
      context: ./repos/ozone/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: ['sh', '-c', 'set -a; . /run/secrets/ozone_secrets; set +a; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; exec node ./service']
    ports:
      - target: 3005
        published: 3005
        mode: host
    volumes:
      # - ./repos/social-app/submodules/atproto/services/ozone/api.js:/app/services/ozone/api.js:ro
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    environment:
      # https://github.com/bluesky-social/ozone/blob/main/HOSTING.md
      - LOG_ENABLED=1
      - DEBUG_MODE=1
      - LOG_DESTINATION=1
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - GOINSECURE=${GOINSECURE}
      - OZONE_DEV_MODE=true
      - OZONE_PORT=3005
      - OZONE_SERVER_DID=${OZONE_SERVER_DID-}
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_ADMIN_HANDLE=${OZONE_ADMIN_HANDLE}
      - OZONE_ADMIN_DIDS=${OZONE_ADMIN_DIDS-}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DB_MIGRATE=1
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - NEXT_PUBLIC_PLC_DIRECTORY_URL=https://${plcFQDN}
      - NEXT_PUBLIC_OZONE_SERVICE_DID=${OZONE_SERVER_DID-}
      - NEXT_PUBLIC_OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - NEXT_PUBLIC_SOCIAL_APP_DOMAIN=${socialappFQDN}
      - NEXT_PUBLIC_SOCIAL_APP_URL=https://${socialappFQDN}
      - NEXT_PUBLIC_HANDLE_RESOLVER_URL=https://${publicApiFQDN}
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
    secrets:
      - ozone_secrets
    restart: always
    depends_on:
      - database
      - caddy

  ozone:
    # moderation-api:
    image: ${BRANDED_NAMESPACE}/bluesky-atproto-ozone:${branded_asof:-latest}
    # profiles: ['', 'atproto']
    build:
      context: ./repos/social-app/submodules/atproto/
      dockerfile: services/ozone/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: ['sh', '-c', 'set -a; . /run/secrets/ozone_secrets; set +a; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; exec node --enable-source-maps api.js']
    ports:
      - target: 3006
        published: 3006
        mode: host
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OZONE_ADMIN_DIDS=
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DEV_MODE=true
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - OZONE_MODERATOR_DIDS=did:web:${bskyFQDN}
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
      - OZONE_PORT=3006
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_SERVER_DID=did:web:${ozoneFQDN}
      - OZONE_TRIAGE_DIDS=
    secrets:
      - ozone_secrets
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - database
      - caddy
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh

  ozone-daemon:
    image: ${BRANDED_NAMESPACE}/bluesky-atproto-ozone:${branded_asof:-latest}
    # profiles: ['', 'atproto']
    build:
      context: ./repos/social-app/submodules/atproto/
      dockerfile: services/ozone/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: ['sh', '-c', 'set -a; . /run/secrets/ozone_secrets; set +a; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; exec node --enable-source-maps daemon.js']
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OZONE_ADMIN_DIDS=
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DEV_MODE=true
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - OZONE_MODERATOR_DIDS=did:web:${bskyFQDN}
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
      - OZONE_PORT=3007
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_SERVER_DID=did:web:${ozoneFQDN}
      - OZONE_TRIAGE_DIDS=
    secrets:
      - ozone_secrets
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - ozone
      - database
      - caddy
    volumes:
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.s

  feed-generator:
    image: ${BRANDED_NAMESPACE}/bluesky-feed-generator:${branded_asof:-latest}
    # profiles: ['', 'feed-generator']
    build:
      context: ./repos/feed-generator/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - target: 3008
        published: 2586
        mode: host
    environment:
      - FEEDGEN_HOSTNAME=${feedgenFQDN}
      - FEEDGEN_LISTENHOST=0.0.0.0
      - FEEDGEN_PORT=3008
      - FEEDGEN_PUBLISHER_DID=${FEEDGEN_PUBLISHER_DID-}
      - FEEDGEN_SERVICE_DID=did:web:${feedgenFQDN}
      - FEEDGEN_SQLITE_LOCATION=/data/db.sqlite
      - FEEDGEN_PLC_URL=https://${plcFQDN}
      - FEEDGEN_SUBSCRIPTION_ENDPOINT=wss://${bgsFQDN}
      - FEEDGEN_SUBSCRIPTION_RECONNECT_DELAY=3000
      - GOINSECURE=${GOINSECURE}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    volumes:
      - feed-generator:/data/
      # - ${rDir}/feed-generator:/app
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:3008/xrpc/_health"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  ipcc:
    image: ${BRANDED_NAMESPACE}/ip-location-service-bsky:${IPCC_IMAGE_TAG:-latest}
    volumes:
      # persist ip location data
      - ipcc:/app/downloads
    environment:
      - DB_TYPE=mmdb
      - COUNTRY=geo-whois-asn-country
      - ASN=
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8080 | grep 'message' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  backup:
    image: creativeprojects/resticprofile:0.32.0
    hostname: backup.${HOST_HOSTNAME:-${DOMAIN}}
    entrypoint: ['/bin/sh', '-c', 'set -a; . /run/secrets/backup_secrets; set +a; sh /usr/local/bin/wait-for-ports.sh -t 120 database:5432; /scripts/entrypoint.sh && resticprofile schedule --all && exec crond -f']
    command: []
    volumes:
      - ./selfhost_scripts/backup-scripts:/scripts:ro
      - ./config/backup/:/etc/resticprofile/:ro
      - pds:/data/pds:rw
      - feed-generator:/data/feed-generator:rw
      - opensearch:/data/opensearch:rw
      - bgs:/data/bgs:rw
      - bsky:/data/bsky:rw
      - backup-staging:/staging
      - backup-repo:/restic-repo
      - ./selfhost_scripts/wait-for-ports.sh:/usr/local/bin/wait-for-ports.sh
    environment:
      - TZ=Etc/UTC
      - BACKUP_PATHS=/data/pds/blobs/ /data/bgs/ /data/bsky/ /data/opensearch/
      # Glob pattern for SQLite files to backup
      - SQLITE_PATHS=sqlite/pds:/data/pds/*.sqlite sqlite/feed-generator:/data/feed-generator/*.sqlite
      - PGHOST=database
      - PGPORT=5432
      - PGUSER=${POSTGRES_USER}
      - RESTIC_REPOSITORY=/restic-repo
      # if RESTIC_REMOTE_REPO#N and RESTIC_REMOTE_PASSWORD#N defined, these repos will be initialized and copied to automatically. N=1,2,3
      # passwords are in secrets file though
      - RESTIC_PASSWORD_FILE=/resticprofile/credentials/local-password
      - RESTIC_AUTO_REMOTE=${RESTIC_AUTO_REMOTE}
      - RESTIC_REMOTE_REPO1=${RESTIC_REMOTE_REPO1-}
      - RESTIC_REMOTE_REPO2=${RESTIC_REMOTE_REPO2-}
      - RESTIC_REMOTE_REPO3=${RESTIC_REMOTE_REPO3-}
      - RESTIC_LOGDIR=/var/log
      # if any of these keys are needed for the remote storage services, they can be configured in the environment
      - AWS_ACCESS_KEY_ID=${RESTIC_AWS_ACCESS_KEY_ID-}
      - GOOGLE_PROJECT_ID=${RESTIC_GOOGLE_PROJECT_ID-}
      - GOOGLE_APPLICATION_CREDENTIALS=/resticprofile/credentials/remote/${RESTIC_GOOGLE_APPLICATION_CREDENTIALS:-restic_google_creds.json}
    secrets:
      - backup_secrets
      - source: google_backup_credentials
        target: /resticprofile/credentials/remote/restic_google_creds.json
    depends_on:
      - database
    healthcheck:
      test: ["CMD-SHELL", "which psql && resticprofile all.status"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./config/telemetry/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      # - "./data/prometheus:/prometheus"
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      - target: 9090
        published: 9090
        # host_ip: '127.0.0.1'
        mode: host
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      # - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=200h"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/metrics >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  otel-collector:
    # opentelemetry otel-collector that collects ddtrace and forwards to jaeger
    image: ${BRANDED_NAMESPACE}/otel-collector:${OTEL_IMAGE_TAG:-latest}
    build:
      context: .
      dockerfile: ops/otel-collector-Dockerfile
    volumes:
      - ./config/telemetry/otel-collector.yml:/etc/otelcol/otel-collector.yml:ro
    command:
      - "--config=/etc/otelcol/otel-collector.yml"
      - "--feature-gates=receiver.datadogreceiver.Enable128BitTraceID"
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      # also expose receivers on localhost for when debugging local services
      # OTLP gRPC receiver
      - target: 4317
        published: 4317
        mode: host
      # OTLP HTTP receiver
      - target: 4318
        published: 4318
        mode: host
      # - "8888:8888"   # Prometheus metrics
      # - "8889:8889"   # Prometheus exporter metrics
      # - "13133:13133" # health_check extension
      # ZPages for direct view
      - target: 55679
        published: 55679
        mode: host
        # host_ip: '127.0.0.1'
      # - "14268:14268" # default jaeger thrift_http receiver, but jaeger listens there
      # - "14278:14278" # alternate jaeger thrift_http receiver
      # datadog receiver for services instrumented with ddtrace
      - target: 8126
        published: 8126
        mode: host
    depends_on:
      # - jaeger
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8888/metrics >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 50
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  jaeger:
    image: jaegertracing/jaeger:latest
    volumes:
      - ./config/telemetry/jaeger.yaml:/etc/jaeger/jaeger.yml:ro
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      # - "14250:14250"   # legacy jaeger-agent spans gRPC protobuf
      # - "14268:14268"   # legacy thrift_http receiver and sampling
      # admin port: health check at / and metrics at /metrics
      - target: 14269
        published: 14269
        mode: host
        # host_ip: '127.0.0.1'
      # - "4317:4317"     # otlp collector over gRPC
      # - "4318:4318"     # otlp collector over http
      # - "6831:6831/udp" # legacy thrift in compact thrift protocol
      # - "6832:6832/udp" # legacy thrift in binary thrift protocol
      # - "9411:9411"     # zipkin v1 JSON or Thrift, v2 JSON or protobuf
      # - "13133:13133"   # healthcheck extension
      # Jaeger UI
      - target: 16686
        published: 16686
        mode: host
        # host_ip: '127.0.0.1'
      # - "16685:16685"   # otlp-based protobuf, legacy protobuf read
      # ZPages for direct view of otel-collector info
      - target: 55679
        published: 55680
        mode: host
        # host_ip: '127.0.0.1'
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8888/metrics >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.1
    # profiles: ['logs']
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
      - SERVER_NAME=${logsFQDN}
      - SERVER_HOST=0.0.0.0
      - OPENSEARCH_REQUESTTIMEOUT=60000
    ports:
      - target: 5601
        published: 5601
        mode: host
    volumes:
      - opensearch-dashboards:/usr/share/opensearch-dashboards/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q 'available'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      <<: *deploy-defaults
    networks:
      - foodios-net
    depends_on:
      - opensearch

  fluent-bit:
    image: fluent/fluent-bit:2.2
    # profiles: ['logs']
    user: root
    volumes:
      - ./config/logging/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./config/logging/parsers.conf:/fluent-bit/etc/parsers.conf:ro
      - ./config/logging/add-container-metadata.lua:/fluent-bit/etc/add-container-metadata.lua:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - fluent-bit-storage:/var/log/flb-storage
    environment:
      - OPENSEARCH_HOST=opensearch
      - OPENSEARCH_PORT=9200
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 60s
    networks:
      - foodios-net
    depends_on:
      - opensearch

