volumes:
  caddy-data:
    name: caddy-data
    #   driver: local
    #   driver_opts:
    #     o: bind
    #     device: ${dDir}/caddy
    #     type: none
  caddy-config:
    name: caddy-config
  database:
    name: database
  opensearch:
    name: opensearch
  bgs:
    name: bgs
  relay:
    name: relay
  bsky:
    name: bsky
  feed-generator:
    name: feed-generator
  pds:
    name: pds
  redis:
    name: redis
  jetstream:
    name: jetstream
  backup-staging:
    name: backup-staging
  backup-repo:
    name: backup-repo
  ipcc:
    name: ipcc
# plc:
# palomar:
# social-app:
  opensearch-dashboards:
  fluent-bit-storage:

x-definitions:
  certificate-volumes: &certificate-volumes
    # CA certificates for self-signed, or a dummy directory if that's not being used
    # These can't be used if the service defines its own volumes, but at least it reduces reuse
    volumes:
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      # leave out host ca-certificates as it can't be included from a single file, or will interfere with update-ca-certificates
      # - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/host-ca-certificates.crt:ro

networks:
  default:
    name: ${docker_network}

services:
  caddy:
    #   reverse proxy
    #   cf. https://blog.kurokobo.com/archives/3669#Caddy_acme_server
    image: ${BRANDED_NAMESPACE}/caddy:${CADDY_IMAGE_TAG:-2}
    build:
      context: .
      dockerfile: ops/caddy-Dockerfile
      args:
        CADDY_DNS_PACKAGE_SRC: ${CADDY_DNS_PACKAGE_SRC-github.com/caddy-dns/cloudflare}
    ports:
      # these ports can be overridden in .env if using haproxy to forward requests to caddy
      - target: 80
        published: ${CADDY_HTTP_PORT:-80}
        mode: host
      - target: 443
        published: ${CADDY_HTTPS_PORT:-443}
        mode: host
      - target: 443
        published: ${CADDY_HTTPS_PORT:-443}
        protocol: udp
        mode: host
      # - 9000:9000 # internal CA server; doesn't need to be public
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - DOMAIN=${DOMAIN}
      - EMAIL4CERTS=${EMAIL4CERTS}
      - apiFQDN=${apiFQDN}
      - bgsFQDN=${bgsFQDN}
      - relayFQDN=${relayFQDN}
      - bskyFQDN=${bskyFQDN}
      - cardFQDN=${cardFQDN}
      - embedFQDN=${embedFQDN}
      - feedgenFQDN=${feedgenFQDN}
      - ipFQDN=${ipFQDN}
      - jetstreamFQDN=${jetstreamFQDN}
      - linkFQDN=${linkFQDN}
      - logsFQDN=${logsFQDN}
      - ozoneFQDN=${ozoneFQDN}
      - palomarFQDN=${palomarFQDN}
      - pdsFQDN=${pdsFQDN}
      - plcFQDN=${plcFQDN}
      - publicApiFQDN=${publicApiFQDN}
      - socialappFQDN=${socialappFQDN}
      - haproxyFORWARD=${haproxyFORWARD-}
      - TRUSTED_PROXIES=${TRUSTED_PROXIES-}
      - CADDY_DNS_PROVIDER=${CADDY_DNS_PROVIDER-}
      - CADDY_DNS_API_TOKEN=${CADDY_DNS_API_TOKEN-}
      - CADDY_DNS_RESOLVER=${CADDY_DNS_RESOLVER-}
      - CADDY_DNS_USED=${CADDY_DNS_USED-}
      - LOGS_DISABLED=${LOGS_DISABLED-}
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=caddy
    # this gets created by make and redirects caddy to local debug services if populated
    env_file:
      - config/caddy-dynamic.env
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./config/caddy-dynamic.env:/etc/caddy-dynamic.env:ro
      # CA certificates for serving self-signed. >>>
      - ./certs/root.crt:${CADDY_CERTS_DIR-/tmp/dummy-certs}/root.crt:ro
      - ./certs/root.key:${CADDY_CERTS_DIR-/tmp/dummy-certs}/root.key:ro
      # CA certificates for serving self-signed. <<<
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
      - caddy-data:/data
      - caddy-config:/config
      # the social app's static website. TODO: uncomment when there's something in here to copy
      # - ./${PDS_WEB_DIR:-staticweb}:/socialweb:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://127.0.0.1:2019/metrics"]
      interval: 5s
      timeout: 3s
      retries: 20
    restart: always

  caddy-sidecar:
    # decides which domains caddy should generate certificates for
    image: httpd:2
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
    <<: *certificate-volumes

  test-wss:
    # used in test mode for checking and debugging wss secure web-socket connections
    image: itaru2622/fastapi:trixie
    environment:
      - app=main:app
      - opts=--host 0.0.0.0 --port 8080
    working_dir: /opt/fastapi-samples/3catchall

  test-ws:
    # used in test mode for checking and debugging ws web-socket connections
    image: itaru2622/fastapi:trixie
    environment:
      - app=main:app
      - opts=--host 0.0.0.0 --port 8080
    working_dir: /opt/fastapi-samples/3catchall

  test-indigo:
    # helper image for trying out indigo commands
    image: ${UNBRANDED_NAMESPACE}/bluesky-indigo-tools:${asof}
    profiles: ['', 'indigo']
    build:
       context: ./repos/indigo/
       dockerfile: Dockerfile
       args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: tail -f /dev/null
    env_file:
      - config/bgs-secrets.env
    environment:
      - ATP_PLC_HOST=https://${plcFQDN}
      - DATA_DIR=/data/bigsky
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
    <<: *certificate-volumes

  database:
    image: postgres:16-trixie
    ports:
      - target: 5432
        published: 5432
        mode: host
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - POSTGRES_INITDB_ARGS=-A scram-sha-256 --encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_DB=healthcheck
    env_file:
      - config/db-secrets.env
    volumes:
      - ./config/init-postgres:/docker-entrypoint-initdb.d/:ro
      - database:/var/lib/postgresql/data/
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U pg -d healthcheck >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  pgadmin:
    image: dpage/pgadmin4
    ports:
      - target: 80
        published: 54321
        mode: host
    environment:
      # TODO store in secret
      - PGADMIN_DEFAULT_EMAIL=example@example.com
      - PGADMIN_DEFAULT_PASSWORD=password
    #   <<: *certificate-volumes
    depends_on:
      - database

  redis:
    image: redis:8-bookworm
    volumes:
      - redis:/data/
    restart: always

  opensearch:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-opensearch:${OPENSEARCH_IMAGE_TAG:-${branded_asof:-latest}}
    profiles: ['', 'indigo', 'logs']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/palomar/Dockerfile.opensearch
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
         # cf. https://github.com/opensearch-project/OpenSearch/issues/8215
       - OPENSEARCH_JAVA_OPTS=${OPENSEARCH_JAVA_OPTS-}
    ports:
      - target: 9200
        published: 9200
        mode: host
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - bootstrap.memory_lock=true
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - node.name=os-node
      - plugins.security.disabled=true
      - transport.host=127.0.0.1
    env_file:
      - config/opensearch-secrets.env
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - opensearch:/usr/share/opensearch/data/
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:9200/_cluster/health | grep '\"status\":' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  plc:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-did-method-plc:${PLC_IMAGE_TAG:-${branded_asof:-latest}}
    profiles: ['', 'did-method-plc']
    build:
      context: ./repos/did-method-plc/
      dockerfile: packages/server/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/did-method-plc"
        org.opencontainers.image.description: "${REBRANDING_NAME} DID PLC server"
    user: root
    entrypoint: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; su node -c "dumb-init -- node --enable-source-maps index.js"']
    command: ''
    ports:
      - target: 2582
        published: 2582
        mode: host
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - PORT=2582
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_PROPAGATION_STYLE=tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
    env_file:
      # this contains POSTGRES_USER, POSTGRES_PASSWORD, DATABASE_URL, DB_CREDS_JSON, DB_MIGRATE_CREDS_JSON
      - config/plc-secrets.env
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:2582/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - database
      - caddy
    <<: *certificate-volumes

  pds:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-atproto-pds:${PDS_IMAGE_TAG:-${branded_asof:-latest}}
    command: sh -c '${UPDATE_CERTS_CMD:-true}; node --heapsnapshot-signal=SIGUSR2 --enable-source-maps --require=./tracer.js index.js'
    profiles: ['', 'atproto']
    build:
      context: ./repos/social-app/submodules/atproto/
      dockerfile: services/pds/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/atproto"
        org.opencontainers.image.description: "${REBRANDING_NAME} ATP Personal Data Server (PDS)"
    ports:
      - target: 2583
        published: 2583
        mode: host
    expose:
      - 2583
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root
    user: root
    environment:
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - PDS_BLOBSTORE_DISK_LOCATION=/pds/blobs
      - PDS_BSKY_APP_VIEW_DID=did:web:${bskyFQDN}
      - PDS_BSKY_APP_VIEW_URL=https://${bskyFQDN}
      - PDS_CRAWLERS=https://${relayFQDN}
      - PDS_DATA_DIRECTORY=/pds
      - PDS_DEV_MODE=true
#     - PDS_BLOBSTORE_DISK_TMP_LOCATION=/pds/image/tmp
      - PDS_DID_PLC_URL=https://${plcFQDN}
      - PDS_EMAIL_FROM_ADDRESS=${PDS_EMAIL_FROM_ADDRESS}
      - PDS_EMAIL_IMAGES_BASE_URL=${PDS_EMAIL_IMAGES_BASE_URL}
      - PDS_EMAIL_SMTP_URL=${PDS_EMAIL_SMTP_URL}
      - PDS_ENABLE_DID_DOC_WITH_SESSION=true
      - PDS_HOSTNAME=${pdsFQDN}
      - PDS_INVITE_INTERVAL=${PDS_INVITE_INTERVAL-}
      - PDS_INVITE_REQUIRED=${PDS_INVITE_REQUIRED}
      - PDS_PORT=2583
      # backfill settings
      - PDS_REPO_BACKFILL_LIMIT_MS=307584000000 # 10 years
      - PDS_MAX_SUBSCRIPTION_BUFFER=10000000 # 10 million records
      - PDS_MAX_REPO_IMPORT_SIZE=1000000000 # don't know what this does. 1 billion.

# starts: unset optional env for testing ozone >>>>>>>
#     - PDS_MOD_SERVICE_DID=did:web:${ozoneFQDN}
#     - PDS_MOD_SERVICE_URL=https://${ozoneFQDN}
#     - PDS_REPORT_SERVICE_DID=did:web:${ozoneFQDN}
#     - PDS_REPORT_SERVICE_URL=https://${ozoneFQDN}
# ends: unset optional env for tesing ozone <<<<<<<
      - PDS_SERVICE_DID=did:web:${pdsFQDN}
      - PDS_SOCIAL_APP_DESCRIPTION=${SOCIAL_APP_DESCRIPTION}
      - PDS_SOCIAL_APP_EMOJI=${SOCIAL_APP_EMOJI}
      - PDS_SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - PDS_SOCIAL_APP_URL=${SOCIAL_APP_URL}
      # other links; these are used in the sign-up screen
      - PDS_CONTACT_EMAIL_ADDRESS=${PDS_CONTACT_EMAIL_ADDRESS}
      - PDS_HOME_URL=https://${pdsFQDN}
      - PDS_PRIVACY_POLICY_URL=${SOCIAL_POLICY_BASE_URL}/privacy-policy/
      - PDS_SUPPORT_URL=https://ghost.foodios.social/support/
      - PDS_TERMS_OF_SERVICE_URL=${SOCIAL_POLICY_BASE_URL}/tos/
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_OTEL_ENABLED=true
      # the package.json misconfigures this service as plc-service
      - DD_SERVICE=pds
      - DD_TRACE_PROPAGATION_STYLE=datadog,tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
      - OTEL_PROPAGATORS=datadog,tracecontext,baggage
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    env_file:
      - config/pds-secrets.env
    volumes:
      - pds:/pds
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:2583/xrpc/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - database
      - caddy

  bgs:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-bgs:${BGS_IMAGE_TAG:-${branded_asof:-latest}}
    profiles: ['disabled', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/bigsky/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/indigo"
        org.opencontainers.image.description: "${REBRANDING_NAME} ATP Relay (aka BGS)"
    ports:
      - target: 2470
        published: 2470
        mode: host
      - target: 2471
        published: 2471
        mode: host
    command: "sh -c '${UPDATE_CERTS_CMD:-true}; /bigsky --db-tracing'"
    environment:
      - ATP_PLC_HOST=https://${plcFQDN}
      - DATA_DIR=/data/bigsky
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - RELAY_DEFAULT_REPO_LIMIT=100000
      - RELAY_NEWPDS_PERDAY_LIMIT=500
      # - RELAY_PERSISTER_DIR=/data/bigsky/events
    env_file:
      - config/bgs-secrets.env # Contains BGS_ADMIN_KEY and database URLs
    volumes:
      - bgs:/data/bigsky
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:2470/xrpc/_health | grep 'status.*ok' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - database
      - caddy

  relay:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-relay:${RELAY_IMAGE_TAG:-${branded_asof:-latest}}
    profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/relay/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/indigo"
        org.opencontainers.image.description: "${REBRANDING_NAME} ATP Relay"
    ports:
      - target: 2472
        published: 2472
        mode: host
      - target: 2473
        published: 2473
        mode: host
    command: "sh -c '${UPDATE_CERTS_CMD:-true}; /relay serve --enable-db-tracing --enable-otel-tracing'"
    environment:
      - ATP_PLC_HOST=https://${plcFQDN}
      - DATA_DIR=/data/relay
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - RELAY_DEFAULT_REPO_LIMIT=100000
      - RELAY_NEW_HOSTS_PER_DAY_LIMIT=500
      - RELAY_PERSIST_DIR=/data/relay/events
      - RELAY_REPLAY_WINDOW=87600h
      - RELAY_TRUSTED_DOMAINS="*.${DOMAIN} ${pdsFQDN}"
      - RELAY_API_LISTEN=:2472
      - RELAY_METRICS_LISTEN=:2473
      - RELAY_DISABLE_REQUEST_CRAWL=true
    env_file:
      - config/relay-secrets.env # Contains RELAY_ADMIN_KEY and database URLs
    volumes:
      - relay:/data/relay
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:2472/xrpc/_health | grep 'status.*ok' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - database
      - caddy

  bsky:
    # appview(api)
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-atproto-bsky:${BSKY_IMAGE_TAG:-${branded_asof:-latest}}
    profiles: ['', 'atproto']
    build:
      context: ./repos/social-app/submodules/atproto/
      dockerfile: services/bsky/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/atproto"
        org.opencontainers.image.description: "${REBRANDING_NAME} App View"
    ports:
      - target: 2584
        published: 2584
        mode: host
      # dataplane port # temporarily exposing for debugging
      - target: 3001
        published: 3001
        mode: host
      # bsync port # temporarily exposing for debugging
      - target: 3002
        published: 3002
        mode: host
    expose:
      - 2584
    command: sh -c '${UPDATE_CERTS_CMD:-true} ; node --enable-source-maps api.js'
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root
    user: root
    environment:
      - APPVIEW_ENV_CONTENT_FILE=env-content.${ENV_CONTENT_SUFFIX}${ENV_CONTENT_SUFFIX:+.}json
      - BSKY_BLOB_CACHE_LOC=/cache/
      - BSKY_BSYNC_HTTP_VERSION=1.1
      - BSKY_BSYNC_PORT=3002
      - BSKY_BSYNC_URL=http://bsky:3002
      - BSKY_COURIER_API_KEY=
      - BSKY_COURIER_HTTP_VERSION=
      - BSKY_COURIER_IGNORE_BAD_TLS=
      - BSKY_COURIER_URL=
      - BSKY_DATAPLANE_HTTP_VERSION=1.1
      - BSKY_DATAPLANE_PORT=3001
      - BSKY_DATAPLANE_URLS=http://bsky:3001
      - BSKY_DB_POSTGRES_SCHEMA=bsky
      - BSKY_DID_PLC_URL=https://${plcFQDN}
      - BSKY_LABELS_FROM_ISSUER_DIDS=${BSKY_LABELS_FROM_ISSUER_DIDS}
      - BSKY_PORT=2584
      - BSKY_PUBLIC_URL=https://${bskyFQDN}
      - BSKY_REPO_PROVIDER=wss://${relayFQDN}
      - BSKY_SEARCH_URL=https://${palomarFQDN}
      - BSKY_SERVER_DID=did:web:${bskyFQDN}
      - BSKY_STATSIG_ENV=${BSKY_STATSIG_ENV-}
      - DEBUG_MODE=1
      - DID_PLC_URL=https://${plcFQDN}
      - ENABLE_MIGRATIONS=false
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - MOD_SERVICE_DID=did:web:${ozoneFQDN}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - REDIS_HOST=redis
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_PROPAGATION_STYLE=tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
    env_file:
      - config/bsky-secrets.env
    volumes:
      - bsky:/cache/
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:2584/ | grep 'AppView' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - database
      - redis
      - caddy

  social-app:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-app:${SOCIAL_APP_IMAGE_TAG:-${branded_asof:-latest}}
    # the social-app Dockerfile compiles for this platform
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
       - SENTRY_ORG=${SENTRY_ORG-}
       - SENTRY_PROJECT=${SENTRY_PROJECT-}
       - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT-}
       - EXPO_PUBLIC_SENTRY_DSN=${EXPO_PUBLIC_SENTRY_DSN-}
       - EXPO_PUBLIC_STATSIG_CLIENT_KEY=${EXPO_PUBLIC_STATSIG_CLIENT_KEY-}
       - EXPO_PUBLIC_STATSIG_API_URL=${EXPO_PUBLIC_STATSIG_API_URL-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} Web App"
    ports:
      - target: 8100
        published: 8100
        mode: host
    command: ["/bin/sh", "-c", "${UPDATE_CERTS_CMD:-true}; /usr/bin/bskyweb serve"]
    environment:
      # cf. https://github.com/bluesky-social/social-app/blob/main/bskyweb/example.env and https://github.com/bluesky-social/bsky-docs/issues/63 :
      - EXPO_PUBLIC_ATP_APPVIEW_DID=did:web:${apiFQDN}
      - EXPO_PUBLIC_ATP_APPVIEW_URL=https://${apiFQDN}
      - ATP_APPVIEW_URL=https://${apiFQDN}
      - EXPO_PUBLIC_ATP_PDS_DID=did:web:${pdsFQDN}
      - EXPO_PUBLIC_ATP_PDS_URL=https://${pdsFQDN}
      - EXPO_PUBLIC_ATP_PUBLIC_APPVIEW_URL=https://${publicApiFQDN}
      - EXPO_PUBLIC_BLUESKY_PROXY_DID=did:web:${bskyFQDN}
      - EXPO_PUBLIC_CHAT_PROXY_DID=${dmServiceDID}
      # - BSKY_CANONICAL_INSTANCE=true # Enable if this is the canonical deployment. This enables routes like /ips-v4, /ips-v6, /security.txt and /.well-known/*
      # - BRANDING_FILE=branding.json # This will usually use the branding.json generated by step10 by exporting from branding.yml
      - EXPO_PUBLIC_CORS_ALLOWED_ORIGINS=https://${socialappFQDN}
      - CORS_ALLOWED_ORIGINS=https://${socialappFQDN}
      - EXPO_PUBLIC_GIF_HOST=${gifFQDN}
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - HTTP_ADDRESS=:8100
      - EXPO_PUBLIC_GEOLOCATION_CONFIG_URL=https://${ipFQDN}/config
      - IPCC_HOST=http://ipcc:8080
      - EXPO_PUBLIC_LINK_HOST=${linkFQDN}
      - LINK_HOST=${linkFQDN}
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - EXPO_PUBLIC_OGCARD_URL=https://${cardFQDN}
      - OGCARD_URL=https://${cardFQDN}
      # comma-separated list for index.html
      - PRECONNECT_DOMAINS=https://${pdsFQDN},https://${linkFQDN}
      # we don't yet have one of these
      - EXPO_PUBLIC_PREVIEW_LINK_META_PROXY=
      # - ROBOTS_DISALLOW_ALL=true # enable this flag to disallow crawling
      - SENTRY_AUTH_TOKEN=${SENTRY_AUTH_TOKEN-}
      # - EXPO_PUBLIC_STATIC_CDN_HOST= # optional variable that is used to serve up fonts and icons
      - EXPO_PUBLIC_SOCIAL_APP_ABOUT=https://${pdsFQDN}
      - EXPO_PUBLIC_SOCIAL_APP_HOST=${socialappFQDN}
      - EXPO_PUBLIC_SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - EXPO_PUBLIC_SOCIAL_APP_URL=https://${socialappFQDN}
      - EXPO_PUBLIC_SOCIAL_EMBED_SERVICE=https://${embedFQDN}
      - EXPO_PUBLIC_SOCIAL_HELP_DESK_URL=${SOCIAL_HELP_DESK_URL}
      - EXPO_PUBLIC_SOCIAL_POLICY_BASE_URL=${SOCIAL_POLICY_BASE_URL}
      - EXPO_PUBLIC_STATSIG_CLIENT_KEY=${STATSIG_CLIENT_KEY}
      - EXPO_PUBLIC_STATSIG_API_URL=${STATSIG_API_URL}
      - EXPO_PUBLIC_STATUS_PAGE_URL=${STATUS_PAGE_URL}
      # we don't yet have one of these
      - EXPO_PUBLIC_VIDEO_SERVICE=
      - EXPO_PUBLIC_VIDEO_SERVICE_DID=
      - SOCIAL_APP_SECURITY_EMAIL=${SOCIAL_APP_SECURITY_EMAIL}
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    <<: *certificate-volumes
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:8100 | grep 'API docs at https://atproto.com' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  social-card:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-card:${SOCIAL_CARD_IMAGE_TAG:-${branded_asof:-latest}}
    # the social-app Dockerfile compiles for this platform
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.bskyogcard
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - SOCIAL_APP_NAME=${SOCIAL_APP_NAME-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} Card Service"
    ports: []
      # - 3003:3003
    user: root
    entrypoint: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; su node -c "dumb-init -- node --heapsnapshot-signal=SIGUSR2 --enable-source-maps dist/bin.js"']
    environment:
      - CARD_PORT=3003
      - CARD_APPVIEW_URL=https://${apiFQDN}
      # Checks a req header x-origin-verify and returns 404 if not matching
      - CARD_ORIGIN_VERIFY=
      - SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
    <<: *certificate-volumes
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3003/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  social-embed:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-embed:${SOCIAL_EMBED_IMAGE_TAG:-${branded_asof:-latest}}
    # the social-app Dockerfile compiles for this platform
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.embedr
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} embed web app"
    ports: []
      # - 8101:8101 # listening port
      # - 9090:9090 # metrics port
    entrypoint: ['sh', '-c', 'apt-get install -y wget ; ${UPDATE_CERTS_CMD:-true} ; dumb-init -- /usr/bin/embedr serve']
    command: ''
    environment:
      - ATP_PUBLIC_APPVIEW_URL=https://${publicApiFQDN}
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - HTTP_ADDRESS=:8101
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      # in dev mode, bskyembed reads from its .env.* VITE_ variables which are interpreted into html
      # however, we're actually not using bskyembed/ (ts) but bskyweb/cmd/embedr (go), which doesn't use these
      # we rather use a combination of .env.go-templates which puts "{{ .varname }}" syntax into the templates, for server-side rendering
      # and a /env-config.json URL which is served up using the environment or command-line parameters
      - OGCARD_URL=https://${cardFQDN}
      - SOCIAL_EMBED_SERVICE=https://${embedFQDN}
      - LINK_URL=https://${linkFQDN}
      - SOCIAL_APP_ABOUT=https://${pdsFQDN}/about
      - SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - SOCIAL_APP_URL=https://${socialappFQDN}
      # - SOCIAL_APP_SECURITY_EMAIL=
      # - SOCIAL_APP_SUPPORT_EMAIL=
    <<: *certificate-volumes
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8101/ | grep 'Embed' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  social-link:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME}-social-link:${SOCIAL_LINK_IMAGE_TAG:-${branded_asof:-latest}}
    # the social-app Dockerfile compiles for this platform
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.bskylink
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} Link Service"
    ports: []
      # - 3004:3004 # listening port
    user: root
    entrypoint: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; su node -c "dumb-init -- node --heapsnapshot-signal=SIGUSR2 --enable-source-maps dist/bin.js"']
    environment:
      - LINK_PORT=3004
      - LINK_HOSTNAMES=${linkFQDN}
      - LINK_APP_HOSTNAME=${socialappFQDN}
      # - LINK_DB_POSTGRES_MIGRATION_URL= # only set if you want to migrate from a previous database
      - LINK_DB_POSTGRES_SCHEMA=
      # just use the defaults for the pool
      # - LINK_DB_POSTGRES_POOL_SIZE=
      # - LINK_DB_POSTGRES_POOL_MAX_USES=
      # - LINK_DB_POSTGRES_POOL_IDLE_TIMEOUT_MS=
      - LINK_SAFELINK_ENABLED=0
      - LINK_SAFELINK_PDS_URL=https://${pdsFQDN}
      - LINK_SAFELINK_AGENT_IDENTIFIER=
      - LINK_SAFELINK_AGENT_PASS=
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT:-info}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
    env_file:
      # this contains POSTGRES_USER, POSTGRES_PASSWORD, DATABASE_URL, DB_CREDS_JSON, DB_MIGRATE_CREDS_JSON
      - config/social-link-secrets.env
    <<: *certificate-volumes
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3004/_health | grep 'version' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  palomar:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-palomar:${PALOMAR_IMAGE_TAG:-${branded_asof:-latest}}
    profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/palomar/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
      labels:
        org.opencontainers.image.source: "https://github.com/inconceivableza/social-app"
        org.opencontainers.image.description: "${REBRANDING_NAME} atproto Search Service"
    entrypoint: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; dumb-init -- "$0" "$@"']
    command: ['/palomar', 'run']
    ports:
      - target: 3999
        published: 3999
        mode: host
      # port 3998 is the default metrics port
    environment:
      # refer https://github.com/bluesky-social/indigo/tree/main/cmd/palomar
      - ATP_BGS_HOST=ws://relay:2472
      - ATP_PLC_HOST=http://pds:2583
      # - ES_CERT_FILE=
      - ES_HOSTS=http://opensearch:9200
      - ES_INSECURE_SSL=true
      - ES_USERNAME=admin
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - PALOMAR_BIND=0.0.0.0:3999
      - PALOMAR_DISCOVER_REPOS=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    env_file:
      - config/palomar-secrets.env
    <<: *certificate-volumes
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3999/_health | grep '\"status\":\"ok\"' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - opensearch
      - database
      - caddy

  jetstream:
    image: ${UNBRANDED_NAMESPACE}/bluesky-jetstream:latest
    profiles: ['', 'jetstream']
    build:
      context: ./repos/jetstream/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - target: 6008
        published: 6008
        mode: host
      # - 6009:6009 metrics address
    volumes:
      - jetstream:/data
      # supporting self-signed certificates, easiest way >>>>
      - ./certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt:ro
      # supporting self-signed certificates, easiest way <<<<
    environment:
      - JETSTREAM_WS_URL=wss://${relayFQDN}/xrpc/com.atproto.sync.subscribeRepos
      - JETSTREAM_DATA_DIR=/data
      - JETSTREAM_LISTEN_ADDR=:6008
      - JETSTREAM_METRICS_LISTEN_ADDR=:6009
      - JETSTREAM_LIVENESS_TTL=96h
    restart: always
    depends_on:
      - caddy

  ozone:
    # moderation-api:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-ozone:${branded_asof:-latest}
    profiles: ['', 'ozone']
    build:
      context: ./repos/ozone/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: ['node', './service']
    ports:
      - target: 3005
        published: 3005
        mode: host
    volumes: []
      # - ./repos/social-app/submodules/atproto/services/ozone/api.js:/app/services/ozone/api.js:ro
    environment:
      # https://github.com/bluesky-social/ozone/blob/main/HOSTING.md
      - LOG_ENABLED=1
      - DEBUG_MODE=1
      - LOG_DESTINATION=1
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=production # otherwise this runs in dev mode which requires all sorts of things
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
      - GOINSECURE=${GOINSECURE}
      - OZONE_DEV_MODE=true
      - OZONE_PORT=3005
      - OZONE_SERVER_DID=${OZONE_SERVICE_DID-} # atproto/packages/ozone/ reads this instead of OZONE_SERVICE_DID, go figure...
      - OZONE_SERVICE_DID=${OZONE_SERVICE_DID-}
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_ADMIN_HANDLE=${OZONE_ADMIN_HANDLE}
      - OZONE_ADMIN_DIDS=${OZONE_ADMIN_DIDS-}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DB_MIGRATE=1
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - NEXT_PUBLIC_PLC_DIRECTORY_URL=https://${plcFQDN}
      - NEXT_PUBLIC_OZONE_SERVICE_DID=${OZONE_SERVICE_DID-}
      - NEXT_PUBLIC_OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - NEXT_PUBLIC_SOCIAL_APP_DOMAIN=${socialappFQDN}
      - NEXT_PUBLIC_SOCIAL_APP_URL=https://${socialappFQDN}
      - NEXT_PUBLIC_HANDLE_RESOLVER_URL=https://${publicApiFQDN}
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
    env_file:
      - config/ozone-secrets.env
    restart: always
    depends_on:
      - database
      - caddy

  feed-generator:
    image: ${UNBRANDED_NAMESPACE}/bluesky-feed-generator:latest
    profiles: ['', 'feed-generator']
    build:
      context: ./repos/feed-generator/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - target: 3008
        published: 2586
        mode: host
    environment:
      - FEEDGEN_HOSTNAME=${feedgenFQDN}
      - FEEDGEN_LISTENHOST=0.0.0.0
      - FEEDGEN_PORT=3008
      - FEEDGEN_PUBLISHER_DID=${FEEDGEN_PUBLISHER_DID-}
      - FEEDGEN_SERVICE_DID=did:web:${feedgenFQDN}
      - FEEDGEN_SQLITE_LOCATION=/data/db.sqlite
      - FEEDGEN_PLC_URL=https://${plcFQDN}
      - FEEDGEN_SUBSCRIPTION_ENDPOINT=wss://${relayFQDN}
      - FEEDGEN_SUBSCRIPTION_RECONNECT_DELAY=3000
      - GOINSECURE=${GOINSECURE}
      - NODE_ENV=${NODE_ENV}
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED-}
    <<: *certificate-volumes
    volumes:
      - feed-generator:/data/
      # - ${rDir}/feed-generator:/app
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:3000/xrpc/_health"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  ipcc:
    image: ${BRANDED_NAMESPACE}/ip-location-service-bsky:${IPCC_IMAGE_TAG:-latest}
    volumes:
      # persist ip location data
      - ipcc:/app/downloads
    environment:
      - DB_TYPE=mmdb
      - COUNTRY=geo-whois-asn-country
      - ASN=
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8080 | grep 'message' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  backup:
    image: creativeprojects/resticprofile:0.32.0
    hostname: backup.${HOST_HOSTNAME:-${DOMAIN}}
    entrypoint: ["/bin/sh"]
    command: ['-c', '/scripts/entrypoint.sh && resticprofile schedule --all && crond -f']
    volumes:
      - ./selfhost_scripts/backup-scripts:/scripts:ro
      - ./config/backup/:/etc/resticprofile/:ro
      - ./data/accounts/backup-credentials:/resticprofile/credentials/remote:ro
      - pds:/data/pds:rw
      - feed-generator:/data/feed-generator:rw
      - opensearch:/data/opensearch:rw
      - bgs:/data/bgs:rw
      - relay:/data/relay:rw
      - bsky:/data/bsky:rw
      - backup-staging:/staging
      - backup-repo:/restic-repo
    environment:
      - TZ=Etc/UTC
      - BACKUP_PATHS=/data/pds/blobs/ /data/bgs/ /data/relay/ /data/bsky/ /data/opensearch/
      # Glob pattern for SQLite files to backup
      - SQLITE_PATHS=sqlite/pds:/data/pds/*.sqlite sqlite/feed-generator:/data/feed-generator/*.sqlite
      - PGHOST=database
      - PGPORT=5432
      - PGUSER=${POSTGRES_USER}
      - RESTIC_REPOSITORY=/restic-repo
      # if RESTIC_REMOTE_REPO#N and RESTIC_REMOTE_PASSWORD#N defined, these repos will be initialized and copied to automatically. N=1,2,3
      # passwords are in secrets file though
      - RESTIC_PASSWORD_FILE=/resticprofile/credentials/local-password
      - RESTIC_AUTO_REMOTE=${RESTIC_AUTO_REMOTE}
      - RESTIC_REMOTE_REPO1=${RESTIC_REMOTE_REPO1-}
      - RESTIC_REMOTE_REPO2=${RESTIC_REMOTE_REPO2-}
      - RESTIC_REMOTE_REPO3=${RESTIC_REMOTE_REPO3-}
      - RESTIC_LOGDIR=/var/log
      # if any of these keys are needed for the remote storage services, they can be configured in the environment
      - AWS_ACCESS_KEY_ID=${RESTIC_AWS_ACCESS_KEY_ID-}
      - GOOGLE_PROJECT_ID=${RESTIC_GOOGLE_PROJECT_ID-}
      - GOOGLE_APPLICATION_CREDENTIALS=/resticprofile/credentials/remote/${RESTIC_GOOGLE_APPLICATION_CREDENTIALS:-restic_google_creds.json}
    env_file:
      - config/backup-secrets.env
    depends_on:
      - database
    healthcheck:
      test: ["CMD-SHELL", "which psql && resticprofile all.status"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./config/telemetry/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      # - "./data/prometheus:/prometheus"
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      - target: 9090
        published: 9090
        host_ip: '127.0.0.1'
        mode: host
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      # - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=200h"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/metrics >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  otel-collector:
    # opentelemetry otel-collector that collects ddtrace and forwards to jaeger
    image: ${BRANDED_NAMESPACE}/otel-collector:${OTEL_IMAGE_TAG:-latest}
    build:
      context: .
      dockerfile: ops/otel-collector-Dockerfile
    volumes:
      - ./config/telemetry/otel-collector.yml:/etc/otelcol/otel-collector.yml:ro
    command:
      - "--config=/etc/otelcol/otel-collector.yml"
      - "--feature-gates=receiver.datadogreceiver.Enable128BitTraceID"
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      # also expose receivers on localhost for when debugging local services
      # OTLP gRPC receiver
      - target: 4317
        published: 4317
        mode: host
      # OTLP HTTP receiver
      - target: 4318
        published: 4318
        mode: host
      # - "8888:8888"   # Prometheus metrics
      # - "8889:8889"   # Prometheus exporter metrics
      # - "13133:13133" # health_check extension
      # ZPages for direct view
      - target: 55679
        published: 55679
        mode: host
        host_ip: '127.0.0.1'
      # - "14268:14268" # default jaeger thrift_http receiver, but jaeger listens there
      # - "14278:14278" # alternate jaeger thrift_http receiver
      # datadog receiver for services instrumented with ddtrace
      - target: 8126
        published: 8126
        mode: host
    depends_on:
      # - jaeger
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8888/metrics >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 50
    restart: always

  jaeger:
    image: jaegertracing/jaeger:latest
    volumes:
      - ./config/telemetry/jaeger.yaml:/etc/jaeger/jaeger.yml:ro
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      # - "14250:14250"   # legacy jaeger-agent spans gRPC protobuf
      # - "14268:14268"   # legacy thrift_http receiver and sampling
      # admin port: health check at / and metrics at /metrics
      - target: 14269
        published: 14269
        mode: host
      # - "4317:4317"     # otlp collector over gRPC
      # - "4318:4318"     # otlp collector over http
      # - "6831:6831/udp" # legacy thrift in compact thrift protocol
      # - "6832:6832/udp" # legacy thrift in binary thrift protocol
      # - "9411:9411"     # zipkin v1 JSON or Thrift, v2 JSON or protobuf
      # - "13133:13133"   # healthcheck extension
      # Jaeger UI
      - target: 16686
        published: 16686
        mode: host
      # - "16685:16685"   # otlp-based protobuf, legacy protobuf read
      # ZPages for direct view of otel-collector info
      - target: 55679
        published: 55680
        mode: host
        host_ip: '127.0.0.1'
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:8888/metrics >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:2.11.1
    # log services will be included in the default profile if LOGS_ENABLED is set to a non-empty value
    profiles: ['${LOGS_DISABLED-}', 'logs']
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"]
      - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
      - SERVER_NAME=${logsFQDN}
      - SERVER_HOST=0.0.0.0
      - OPENSEARCH_REQUESTTIMEOUT=60000
    ports:
      - target: 5601
        published: 5601
        mode: host
    volumes:
      - opensearch-dashboards:/usr/share/opensearch-dashboards/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:5601/api/status | grep -q 'available'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      - opensearch
    restart: always

  fluent-bit:
    image: fluent/fluent-bit:2.2
    # log services will be included in the default profile if LOGS_ENABLED is set to a non-empty value
    profiles: ['${LOGS_DISABLED-}', 'logs']
    user: root
    volumes:
      - ./config/logging/fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro
      - ./config/logging/parsers.conf:/fluent-bit/etc/parsers.conf:ro
      - ./config/logging/add-container-metadata.lua:/fluent-bit/etc/add-container-metadata.lua:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - fluent-bit-storage:/var/log/flb-storage
    environment:
      - OPENSEARCH_HOST=opensearch
      - OPENSEARCH_PORT=9200
    depends_on:
      - opensearch
    restart: always

