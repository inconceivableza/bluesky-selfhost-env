volumes:
  caddy-data:
    name: caddy-data
    #   driver: local
    #   driver_opts:
    #     o: bind
    #     device: ${dDir}/caddy
    #     type: none
  caddy-config:
    name: caddy-config
  database:
    name: database
  opensearch:
    name: opensearch
  bgs:
    name: bgs
  bsky:
    name: bsky
  feed-generator:
    name: feed-generator
  pds:
    name: pds
  redis:
    name: redis
  jetstream:
    name: jetstream
  backup-staging:
    name: backup-staging
  backup-repo:
    name: backup-repo
  ipcc:
    name: ipcc
# ozone:
# ozone-daemon:
# plc:
# palomar:
# social-app:

x-definitions:
  certificate-volumes: &certificate-volumes
    # CA certificates for self-signed, or a dummy directory if that's not being used
    # These can't be used if the service defines its own volumes, but at least it reduces reuse
    volumes:
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro

networks:
 default:
    name: ${docker_network}

services:
  caddy:
    #   reverse proxy
    #   cf. https://blog.kurokobo.com/archives/3669#Caddy_acme_server
    image: ${BRANDED_NAMESPACE}/caddy:2
    build:
      context: .
      dockerfile: ops/caddy-Dockerfile
      args:
        CADDY_DNS_PACKAGE_SRC: ${CADDY_DNS_PACKAGE_SRC-}
    ports:
      - 80:80
      - 443:443
      - 443:443/udp
      # - 9000:9000 # internal CA server; doesn't need to be public
    env_file: config/bsky-secrets.env
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - DOMAIN=${DOMAIN}
      - socialappFQDN=${socialappFQDN}
      - pdsFQDN=${pdsFQDN}
      - EMAIL4CERTS=${EMAIL4CERTS}
      - bgsFQDN=${bgsFQDN}
      - bskyFQDN=${bskyFQDN}
      - feedgenFQDN=${feedgenFQDN}
      - jetstreamFQDN=${jetstreamFQDN}
      - ozoneFQDN=${ozoneFQDN}
      - palomarFQDN=${palomarFQDN}
      - pdsFQDN=${pdsFQDN}
      - plcFQDN=${plcFQDN}
      - publicApiFQDN=${publicApiFQDN}
      - socialappFQDN=${socialappFQDN}
      - CADDY_DNS_PROVIDER=${CADDY_DNS_PROVIDER-}
      - CADDY_DNS_API_TOKEN=${CADDY_DNS_API_TOKEN-}
      - CADDY_DNS_RESOLVER=${CADDY_DNS_RESOLVER-}
      - CADDY_DNS_USED=${CADDY_DNS_USED-}
      - OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=caddy
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile
      # CA certificates for serving self-signed. >>>
      - ./certs/root.crt:/data/caddy/pki/authorities/local/root.crt:ro
      - ./certs/root.key:/data/caddy/pki/authorities/local/root.key:ro
      # CA certificates for serving self-signed. <<<
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
      # <<<
      - caddy-data:/data
      - caddy-config:/config
      # the social app's static website
      - ./${PDS_WEB_DIR:-staticweb}:/socialweb
    healthcheck:
      # https://caddy.community/t/what-is-the-best-practise-for-doing-a-health-check-for-caddy-containers/12995
      test: "wget --no-verbose --tries=1 --spider http://127.0.0.1:2019/metrics || exit 1"
      interval: 5s
      retries: 20
    restart: always

# to generate HTTPS certifications on-demand >>>>>
  caddy-sidecar:
    image: httpd:2
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    <<: *certificate-volumes
# to generate HTTPS certifications on-demand <<<<<

# debug for caddy>>> 
  test-wss:
    image: itaru2622/fastapi:bookworm
    environment:
      - app=main:app
      - opts=--host 0.0.0.0 --port 8080
    working_dir: /opt/fastapi-samples/3catchall

  test-ws:
    image: itaru2622/fastapi:bookworm
    environment:
      - app=main:app
      - opts=--host 0.0.0.0 --port 8080
    working_dir: /opt/fastapi-samples/3catchall

# debug for caddy <<<
# debug for bluesky with indigo subcmds >>>
  test-indigo:
    image: ${UNBRANDED_NAMESPACE}/bluesky-indigo-tools:${asof}
    profiles: ['', 'indigo']
    build:
       context: ./repos/indigo/
       dockerfile: Dockerfile
       args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: tail -f /dev/null
    env_file: config/bgs-secrets.env
    environment:
      - ATP_PLC_HOST=https://${plcFQDN}
      - DATA_DIR=/data/bigsky
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    <<: *certificate-volumes

# debug for bluesky with indigo subcmds <<<

  database:
    image: postgres:16-bookworm
    ports:
      - 5432:5432
    env_file:
      - config/db-secrets.env
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_DB=healthcheck
    volumes:
      - ./config/init-postgres:/docker-entrypoint-initdb.d/
      - database:/var/lib/postgresql/data/
    restart: always

  pgadmin:
    image: dpage/pgadmin4
    ports:
      - 127.0.0.1:54321:80
    environment:
      - PGADMIN_DEFAULT_EMAIL=example@example.com
      - PGADMIN_DEFAULT_PASSWORD=password
      #   <<: *certificate-volumes
    depends_on:
      - database

  redis:
    image: redis:7-bookworm
    volumes:
      - redis:/data/
    restart: always

  opensearch:
    image: ${UNBRANDED_NAMESPACE}/bluesky-indigo-opensearch:${asof}
    profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/palomar/Dockerfile.opensearch
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
# cf. https://github.com/opensearch-project/OpenSearch/issues/8215
       - OPENSEARCH_JAVA_OPTS=${OPENSEARCH_JAVA_OPTS-}
    ports:
      - 9200:9200
    env_file: config/opensearch-secrets.env
    environment:
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
      - bootstrap.memory_lock=true
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - http.host=0.0.0.0
      - node.name=os-node
      - plugins.security.disabled=true
      - transport.host=127.0.0.1
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - opensearch:/usr/share/opensearch/data/
    restart: always

  plc:
#   image: ${UNBRANDED_NAMESPACE}/bluesky-did-method-plc:${asof}
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-did-method-plc:${branded_asof}
    profiles: ['', 'did-method-plc']
    build:
      context: ./repos/did-method-plc/
      dockerfile: packages/server/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    user: root
    entrypoint: ['sh', '-c', '${UPDATE_CERTS_CMD:-true}; su node -c "dumb-init -- node --enable-source-maps index.js"']
    command: ''
    ports:
      - 2582:2582
    <<: *certificate-volumes
    env_file:
      # this contains POSTGRES_USER, POSTGRES_PASSWORD, DATABASE_URL, DB_CREDS_JSON, DB_MIGRATE_CREDS_JSON
      - config/plc-secrets.env
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PORT=2582
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_PROPAGATION_STYLE=tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
    restart: always
    depends_on:
      - database
      - caddy

  pds:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-atproto-pds:${asof}
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-atproto-pds:${branded_asof}
    command: sh -c '${UPDATE_CERTS_CMD:-true}; node --heapsnapshot-signal=SIGUSR2 --enable-source-maps --require=./tracer.js index.js'
    profiles: ['', 'atproto']
    build:
      context: ./repos/atproto/
      dockerfile: services/pds/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 2583:2583
    expose:
      - 2583
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root. >>>>
    user: root
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root. <<<<
    env_file: config/pds-secrets.env
    environment:
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PDS_BLOBSTORE_DISK_LOCATION=/pds/blobs
      - PDS_BSKY_APP_VIEW_DID=did:web:${bskyFQDN}
      - PDS_BSKY_APP_VIEW_URL=https://${bskyFQDN}
      - PDS_CRAWLERS=https://${bgsFQDN}
      - PDS_DATA_DIRECTORY=/pds
      - PDS_DEV_MODE=true
#     - PDS_BLOBSTORE_DISK_TMP_LOCATION=/pds/image/tmp
      - PDS_DID_PLC_URL=https://${plcFQDN}
      - PDS_EMAIL_FROM_ADDRESS=${PDS_EMAIL_FROM_ADDRESS}
      - PDS_EMAIL_IMAGES_BASE_URL=${PDS_EMAIL_IMAGES_BASE_URL}
      - PDS_EMAIL_SMTP_URL=${PDS_EMAIL_SMTP_URL}
      - PDS_ENABLE_DID_DOC_WITH_SESSION=true
      - PDS_HOSTNAME=${pdsFQDN}
      - PDS_INVITE_INTERVAL=${PDS_INVITE_INTERVAL-}
      - PDS_INVITE_REQUIRED=${PDS_INVITE_REQUIRED}
      - PDS_PORT=2583
# starts: unset optional env for tesing ozone >>>>>>>
#     - PDS_MOD_SERVICE_DID=did:web:${ozoneFQDN}
#     - PDS_MOD_SERVICE_URL=https://${ozoneFQDN}
#     - PDS_REPORT_SERVICE_DID=did:web:${ozoneFQDN}
#     - PDS_REPORT_SERVICE_URL=https://${ozoneFQDN}
# ends: unset optional env for tesing ozone <<<<<<<
      - PDS_SERVICE_DID=did:web:${pdsFQDN}
      - PDS_SOCIAL_APP_DESCRIPTION=${SOCIAL_APP_DESCRIPTION}
      - PDS_SOCIAL_APP_EMOJI=${SOCIAL_APP_EMOJI}
      - PDS_SOCIAL_APP_NAME=${SOCIAL_APP_NAME}
      - PDS_SOCIAL_APP_URL=${SOCIAL_APP_URL}
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_OTEL_ENABLED=true
      - DD_SERVICE=pds # the package.json misconfigures this as plc-service
      - DD_TRACE_PROPAGATION_STYLE=datadog,tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
      - OTEL_PROPAGATORS=datadog,tracecontext,baggage
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    volumes:
      - pds:/pds
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    restart: always
    depends_on:
      - database
      - caddy

  bgs:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-bgs:${branded_asof}
    profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/bigsky/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 2470:2470
      - 2471:2471
    command: "sh -c '${UPDATE_CERTS_CMD:-true}; /bigsky --db-tracing'"
    env_file: config/bgs-secrets.env # Contains BGS_ADMIN_KEY and database URLs
    environment:
      - ATP_PLC_HOST=https://${plcFQDN}
      - DATA_DIR=/data/bigsky
      - DEBUG_MODE=1
      - GOINSECURE=${GOINSECURE}
#     - GODEBUG=netdns=go+4
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - RELAY_NEWPDS_PERDAY_LIMIT=10000
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    volumes:
      - bgs:/data/bigsky
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    healthcheck:
      test: ["CMD-SHELL", "wget -q -O- http://localhost:2470/xrpc/_health | grep 'status.*ok' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always
    depends_on:
      - database
      - caddy

  bsky:
    # appview(api)
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-atproto-bsky:${branded_asof}
    profiles: ['', 'atproto']
    build:
      context: ./repos/atproto/
      dockerfile: services/bsky/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 2584:2584
    expose:
      - 2584
    command: sh -c '${UPDATE_CERTS_CMD:-true} ; node --enable-source-maps api.js'
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root. >>>>
    user: root
    # to fix permision mismatch between volume owner(root) and process user(uid:1000, i.e: node), run as root. <<<<
    env_file: config/bsky-secrets.env
    environment:
      - BSKY_BLOB_CACHE_LOC=/cache/
      - BSKY_BSYNC_HTTP_VERSION=1.1
      - BSKY_BSYNC_PORT=3002
      - BSKY_BSYNC_URL=http://bsky:3002
      - BSKY_COURIER_URL=http://fake-courier.example.invalid/
      - BSKY_DATAPLANE_HTTP_VERSION=1.1
      - BSKY_DATAPLANE_PORT=3001
      - BSKY_DATAPLANE_URLS=http://bsky:3001
      - BSKY_DB_POSTGRES_SCHEMA=bsky
      - BSKY_DID_PLC_URL=https://${plcFQDN}
      - BSKY_LABELS_FROM_ISSUER_DIDS=${BSKY_LABELS_FROM_ISSUER_DIDS}
      - BSKY_PUBLIC_URL=https://${bskyFQDN}
      - BSKY_REPO_PROVIDER=wss://${bgsFQDN}
      - BSKY_SEARCH_URL=https://${palomarFQDN}
      - BSKY_SERVER_DID=did:web:${bskyFQDN}
      - BSKY_STATSIG_ENV=${BSKY_STATSIG_ENV-}
      - DEBUG_MODE=1
      - DID_PLC_URL=https://${plcFQDN}
      - ENABLE_MIGRATIONS=false
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - MOD_SERVICE_DID=did:web:${ozoneFQDN}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PORT=2584
      - REDIS_HOST=redis
      # telemetry settings
      - DD_TRACE_AGENT_URL=http://otel-collector:8126
      - DD_TRACE_PROPAGATION_STYLE=tracecontext,baggage
      - DD_TRACE_128_BIT_TRACEID_LOGGING_ENABLED=true
    volumes:
      - bsky:/cache/
      # CA certificates for connecting to self-signed, or a dummy directory if not used
      - ./certs/root.crt:${CUSTOM_CERTS_DIR}/root.crt:ro
      - ./certs/intermediate.crt:${CUSTOM_CERTS_DIR}/intermediate.crt:ro
      - ./certs/ca-certificates.crt:${CUSTOM_CERTS_DIR}/ca-certificates.crt:ro
    restart: always
    depends_on:
      - database
      - redis
      - caddy

  social-app:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-social-app:${asof}
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-app:${branded_asof}-${DOMAIN}
    # the social-app Dockerfile compiles for this platform
    platform: linux/amd64
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
       - SENTRY_ORG=${SENTRY_ORG-}
       - SENTRY_PROJECT=${SENTRY_PROJECT-}
       - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT-}
       - EXPO_PUBLIC_SENTRY_DSN=${EXPO_PUBLIC_SENTRY_DSN-}
       - EXPO_PUBLIC_STATSIG_CLIENT_KEY=${EXPO_PUBLIC_STATSIG_CLIENT_KEY-}
       - EXPO_PUBLIC_STATSIG_API_URL=${EXPO_PUBLIC_STATSIG_API_URL-}
    ports:
      - 8100:8100
    command: /bin/sh -c "${UPDATE_CERTS_CMD:-true} ; /usr/bin/$${REBRANDED_WEB_SERVICE:-bskyweb} serve"
    <<: *certificate-volumes
    environment:
      # cf. https://github.com/bluesky-social/social-app/blob/main/bskyweb/example.env and https://github.com/bluesky-social/bsky-docs/issues/63 :
      - ATP_APPVIEW_HOST=https://${apiFQDN}
      - ATP_PDS_HOST=https://${pdsFQDN}
      - ATP_PUBLIC_APPVIEW_HOST=https://${publicApiFQDN}
      # - BSKY_CANONICAL_INSTANCE=true # Enable if this is the canonical deployment. This enables routes like /ips-v4, /ips-v6, /security.txt and /.well-known/*
      - CORS_ALLOWED_ORIGINS="https://${socialappFQDN}" # defaults are bsky.app main.bsky.dev and app.staging.bsky.dev
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - HTTP_ADDRESS=:8100
      - IPCC_HOST=http://ipcc:8080
      - LINK_HOST=https://${linkFQDN}
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OGCARD_HOST=${cardFQDN}
      - REBRANDED_WEB_SERVICE=${REBRANDED_WEB_SERVICE:-bskyweb}
      # - ROBOTS_DISALLOW_ALL=true # enable this flag to disallow crawling
      - SENTRY_AUTH_TOKEN=${SENTRY_AUTH_TOKEN-}
      # - STATIC_CDN_HOST= # optional variable that is used to serve up fonts and icons
      - STATUS_PAGE_URL=${STATUS_PAGE_URL}
      - SOCIAL_APP_URL=https://${socialappFQDN}
      - SOCIAL_EMBED_SERVICE=https://${embedFQDN} # TODO: actually publish the embed service on a domain
      - SOCIAL_HELP_DESK_URL=${SOCIAL_HELP_DESK_URL}
      - SOCIAL_POLICY_BASE_URL=${SOCIAL_POLICY_BASE_URL}
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:8100 | grep 'API docs at https://atproto.com' >/dev/null"]
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  social-card:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-social-app:${asof}
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-card:${branded_asof}-${DOMAIN}
    # the social-app Dockerfile compiles for this platform
    platform: linux/amd64
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.bskyogcard
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
    ports: []
      # - 3000:3000 # this is the default port it listens on, same as ozone and feedgen
      # - 3002:3002 # this is what we're configuring it to listen on
    command: /bin/sh -c "${UPDATE_CERTS_CMD:-true} ; node --heapsnapshot-signal=SIGUSR2 --enable-source-maps dist/bin.js"
    <<: *certificate-volumes
    env_file:
      # this contains POSTGRES_USER, POSTGRES_PASSWORD, DATABASE_URL, DB_CREDS_JSON, DB_MIGRATE_CREDS_JSON
      - config/social-link-secrets.env
    environment:
      - CARD_PORT=3002
      - CARD_VERSION=${SOCIAL_CARD_VERSION:-unknown} # this should be the git commit, generated by the make build command
      - CARD_APPVIEW_URL=https://${apiFQDN}
      - CARD_ORIGIN_VERIFY= # Checks a req header x-origin-verify and returns 404 if not matching
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:3003/_health | grep 'version' >/dev/null"] 
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  social-embed:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-social-app:${asof}
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-embed:${branded_asof}-${DOMAIN}
    # the social-app Dockerfile compiles for this platform
    platform: linux/amd64
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.embedr
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
    ports: []
      # - 8100:8100 # this is the default port it listens on, same as social-app
      # - 8101:8101 # this is the default port we're telling it to listen on
      # - 9090:9090 # this is the metrics port
    command: /bin/sh -c "${UPDATE_CERTS_CMD:-true} ; /usr/bin/embedr serve"
    <<: *certificate-volumes
    environment:
      - ATP_APPVIEW_HOST=https://${apiFQDN}
      - ATP_PUBLIC_APPVIEW_HOST=https://${publicApiFQDN}
      - GOINSECURE=${GOINSECURE}
      - GOLOG_LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - HTTP_ADDRESS=:8101
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      # these are passed through to bskyembed's vite which interpolates them. It's yucky and in HTML doesn't support fallback
      # however, we're actually not using bskyembed/ (ts) but bskyweb/cmd/embedr (go), which doesn't use these
      - VITE_CARD_HOST=${cardFQDN}
      - VITE_EMBED_URL=${embedFQDN}
      - VITE_LINK_HOST=${linkFQDN}
      - VITE_PUBLIC_APPVIEW_HOST=https://${publicApiFQDN}
      - VITE_SOCIAL_APP_HOST=${socialappFQDN}
      - VITE_SOCIAL_APP_ABOUT=https://${pdsFQDN}/about
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:8101/ | grep 'Embed' >/dev/null"] 
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  social-link:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-social-app:${asof}
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-social-link:${branded_asof}-${DOMAIN}
    # the social-app Dockerfile compiles for this platform
    platform: linux/amd64
    profiles: ['', 'social-app']
    build:
      context: ./repos/social-app/
      dockerfile: Dockerfile.bskylink
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
    ports: []
      # - 3000:3000 # this is the default port it listens on, same as ozone and feedgen
      # - 3001:3001 # this is what we're configuring it to listen on
    command: /bin/sh -c "${UPDATE_CERTS_CMD:-true} ; node --heapsnapshot-signal=SIGUSR2 --enable-source-maps dist/bin.js"
    <<: *certificate-volumes
    env_file:
      # this contains POSTGRES_USER, POSTGRES_PASSWORD, DATABASE_URL, DB_CREDS_JSON, DB_MIGRATE_CREDS_JSON
      - config/social-link-secrets.env
    environment:
      - LINK_PORT=3001
      - LINK_VERSION=${SOCIAL_LINK_VERSION:-unknown} # this should be the git commit, generated by the make build command
      - LINK_HOSTNAMES=${linkFQDN} # ?? should this have more?
      - LINK_APP_HOSTNAME=${socialappFQDN}
      # - LINK_DB_POSTGRES_URL= # this is in the secrets file
      - LINK_DB_POSTGRES_MIGRATION_URL= # only set if you want to migrate from a previous database
      - LINK_DB_POSTGRES_SCHEMA= # presumably this will use the default
      # just use the defaults for the pool
      # - LINK_DB_POSTGRES_POOL_SIZE=
      # - LINK_DB_POSTGRES_POOL_MAX_USES=
      # - LINK_DB_POSTGRES_POOL_IDLE_TIMEOUT_MS=
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:3001/_health | grep 'version' >/dev/null"] 
      timeout: 10s
      interval: 10s
      retries: 5
    restart: always

  palomar:
    image: ${BRANDED_NAMESPACE}/${REBRANDING_NAME:-bluesky}-indigo-palomar:${branded_asof}
    profiles: ['', 'indigo']
    build:
      context: ./repos/indigo/
      dockerfile: cmd/palomar/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 3999:3999
      # - 3998:3998 # metrics port by default
    <<: *certificate-volumes
    env_file: config/palomar-secrets.env
    environment:
      # refer https://github.com/bluesky-social/indigo/tree/main/cmd/palomar
      - ATP_BGS_HOST=wss://${bgsFQDN}
      - ATP_PLC_HOST=https://${plcFQDN}
#     - ES_CERT_FILE=
      - ES_HOSTS=http://opensearch:9200
      - ES_INSECURE_SSL=true
      - ES_USERNAME=admin
      - GOINSECURE=${GOINSECURE}
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PALOMAR_BIND=0.0.0.0:3999
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
    restart: always
    depends_on:
      - opensearch
      - database
      - caddy

  jetstream:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-jetstream:${asof}
    image: ${UNBRANDED_NAMESPACE}/bluesky-jetstream:latest
    profiles: ['', 'jetstream']
    build:
      context: ./repos/jetstream/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 6008:6008
      # - 6009:6009 metrics address
    volumes:
      - jetstream:/data
      # supporting self-signed certificates, easiest way >>>>
      - ./certs/ca-certificates.crt:/etc/ssl/certs/ca-certificates.crt:ro
      # supporting self-signed certificates, easiest way <<<<
    environment:
      - JETSTREAM_WS_URL=wss://${bgsFQDN}/xrpc/com.atproto.sync.subscribeRepos
      - JETSTREAM_DATA_DIR=/data
      - JETSTREAM_LISTEN_ADDR=:6008
      - JETSTREAM_METRICS_LISTEN_ADDR=:6009
      - JETSTREAM_LIVENESS_TTL=96h
    restart: always
    depends_on:
      - caddy

  ozone-standalone:
    # moderation-api:
    image: ${UNBRANDED_NAMESPACE}/bluesky-ozone:latest
    profiles: ['', 'ozone']
    build:
      context: ./repos/ozone/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 3000:3000
    <<: *certificate-volumes
    volumes: []
#     - ./repos/atproto/services/ozone/api.js:/app/services/ozone/api.js:ro
    env_file: config/ozone-secrets.env
    environment:
#     https://github.com/bluesky-social/ozone/blob/main/HOSTING.md
      - LOG_ENABLED=1
      - DEBUG_MODE=1
      - LOG_DESTINATION=1
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
#     - NODE_ENV=development
      - NODE_ENV=production
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - GOINSECURE=${GOINSECURE}
      - OZONE_DEV_MODE=true
      - OZONE_PORT=3000
      - OZONE_SERVER_DID=${OZONE_SERVER_DID-}
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_ADMIN_HANDLE=${OZONE_ADMIN_HANDLE}
      - OZONE_ADMIN_DIDS=${OZONE_ADMIN_DIDS-}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DB_MIGRATE=1
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - NEXT_PUBLIC_PLC_DIRECTORY_URL=https://${plcFQDN}
      - NEXT_PUBLIC_OZONE_SERVICE_DID=${OZONE_SERVER_DID-}
      - NEXT_PUBLIC_OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - NEXT_PUBLIC_SOCIAL_APP_DOMAIN=${socialappFQDN}
      - NEXT_PUBLIC_SOCIAL_APP_URL=https://${socialappFQDN}
      - NEXT_PUBLIC_HANDLE_RESOLVER_URL=https://${publicApiFQDN}
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
    restart: always
    depends_on:
      - database
      - caddy

  ozone:
    # moderation-api:
    image: ${UNBRANDED_NAMESPACE}/bluesky-atproto-ozone:${asof}
    profiles: ['', 'atproto']
    build:
      context: ./repos/atproto/
      dockerfile: services/ozone/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: node --enable-source-maps api.js
    ports:
      - 3000:3000
    <<: *certificate-volumes
#    volumes:
#      - ./repos/atproto/services/ozone/api.js:/app/services/ozone/api.js:ro
    env_file: config/ozone-secrets.env
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OZONE_ADMIN_DIDS=
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DEV_MODE=true
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - OZONE_MODERATOR_DIDS=did:web:${bskyFQDN}
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
      - OZONE_PORT=3000
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_SERVER_DID=did:web:${ozoneFQDN}
      - OZONE_TRIAGE_DIDS=
    restart: always
    depends_on:
      - database
      - caddy

  ozone-daemon:
    image: ${UNBRANDED_NAMESPACE}/bluesky-atproto-ozone:${asof}
    profiles: ['', 'atproto']
    build:
      context: ./repos/atproto/
      dockerfile: services/ozone/Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    command: node --enable-source-maps daemon.js
    <<: *certificate-volumes
    env_file: config/ozone-secrets.env
    environment:
      - DEBUG_MODE=1
      - ENABLE_MIGRATIONS=true
      - GOINSECURE=${GOINSECURE}
      - LOG_DESTINATION=1
      - LOG_ENABLED=true
      - LOG_LEVEL=${LOG_LEVEL_DEFAULT}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - OZONE_ADMIN_DIDS=
      - OZONE_APPVIEW_DID=did:web:${bskyFQDN}
      - OZONE_APPVIEW_PUSH_EVENTS=true
      - OZONE_APPVIEW_URL=https://${bskyFQDN}
      - OZONE_DB_POSTGRES_SCHEMA=ozone
      - OZONE_DEV_MODE=true
      - OZONE_DID_PLC_URL=https://${plcFQDN}
      - OZONE_MODERATOR_DIDS=did:web:${bskyFQDN}
      - OZONE_PDS_DID=did:web:${pdsFQDN}
      - OZONE_PDS_URL=https://${pdsFQDN}
      - OZONE_PORT=3000
      - OZONE_PUBLIC_URL=https://${ozoneFQDN}
      - OZONE_SERVER_DID=did:web:${ozoneFQDN}
      - OZONE_TRIAGE_DIDS=
    restart: always
    depends_on:
      - ozone
      - database
      - caddy

  feed-generator:
    # image: ${UNBRANDED_NAMESPACE}/bluesky-feed-generator:${asof}
    image: ${UNBRANDED_NAMESPACE}/bluesky-feed-generator:latest
    profiles: ['', 'feed-generator']
    build:
      context: ./repos/feed-generator/
      dockerfile: Dockerfile
      args:
       - http_proxy=${http_proxy-}
       - https_proxy=${https_proxy-}
       - no_proxy=${no_proxy-}
       - JAVA_TOOL_OPTIONS=${JAVA_TOOL_OPTIONS-}
    ports:
      - 2586:3000
    environment:
      - FEEDGEN_HOSTNAME=${feedgenFQDN}
      - FEEDGEN_LISTENHOST=0.0.0.0
      - FEEDGEN_PORT=3000
      - FEEDGEN_PUBLISHER_DID=${FEEDGEN_PUBLISHER_DID-}
      - FEEDGEN_SERVICE_DID=did:web:${feedgenFQDN}
      - FEEDGEN_SQLITE_LOCATION=/data/db.sqlite
      - FEEDGEN_PLC_URL=https://${plcFQDN}
      - FEEDGEN_SUBSCRIPTION_ENDPOINT=wss://${bgsFQDN}
      - FEEDGEN_SUBSCRIPTION_RECONNECT_DELAY=3000
      - GOINSECURE=${GOINSECURE}
      - NODE_ENV=development
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/ca-certificates.crt # this is what it should read whether it's been updated for custom certificates or not
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "curl -s http://localhost:3000/xrpc/_health"]
      timeout: 10s
      interval: 10s
      retries: 5
    <<: *certificate-volumes
    volumes:
      - feed-generator:/data/
#     - ${rDir}/feed-generator:/app

  ipcc:
    image: davidfinconceivableza/ip-location-service-bsky
    volumes:
      # persist ip location data
      - ipcc:/app/downloads
    environment:
      - DB_TYPE=mmdb
      - COUNTRY=geo-whois-asn-country
      - ASN=
    restart: always

  backup:
    image: creativeprojects/resticprofile:0.29.0 # wait until restic issue #5324 fix is released before upgrading
    hostname: backup.${HOST_HOSTNAME:-${DOMAIN}}
    volumes:
      - ./selfhost_scripts/backup-scripts:/scripts:ro
      - ./config/backup/:/etc/resticprofile/:ro
      - ./data/accounts/backup-credentials:/resticprofile/credentials/remote:ro
      - pds:/data/pds:rw
      - feed-generator:/data/feed-generator:rw
      - opensearch:/data/opensearch:rw
      - bgs:/data/bgs:rw
      - bsky:/data/bsky:rw
      - backup-staging:/staging
      - backup-repo:/restic-repo
    env_file: config/backup-secrets.env
    environment:
      - TZ=Etc/UTC
      - BACKUP_PATHS=/data/pds/blobs/ /data/bgs/ /data/bsky/ /data/opensearch/
      - SQLITE_PATHS=sqlite/pds:/data/pds/*.sqlite sqlite/feed-generator:/data/feed-generator/*.sqlite  # Glob pattern for SQLite files
      - PGHOST=database
      - PGPORT=5432
      - PGUSER=${POSTGRES_USER}
      - RESTIC_REPOSITORY=/restic-repo
      # if RESTIC_REMOTE_REPO#N and RESTIC_REMOTE_PASSWORD#N defined, these repos will be initialized and copied to automatically. N=1,2,3
      # passwords are in secrets file though
      - RESTIC_PASSWORD_FILE=/resticprofile/credentials/local-password
      - RESTIC_REMOTE_REPO1=${RESTIC_REMOTE_REPO1-}
      - RESTIC_REMOTE_REPO2=${RESTIC_REMOTE_REPO2-}
      - RESTIC_REMOTE_REPO3=${RESTIC_REMOTE_REPO3-}
      - RESTIC_LOGDIR=/var/log
      # if any of these keys are needed for the remote storage services, they can be configured in the environment
      - AWS_ACCESS_KEY_ID=${RESTIC_AWS_ACCESS_KEY_ID-}
      - GOOGLE_PROJECT_ID=${RESTIC_GOOGLE_PROJECT_ID-}
      - GOOGLE_APPLICATION_CREDENTIALS=/resticprofile/credentials/remote/${RESTIC_GOOGLE_APPLICATION_CREDENTIALS-}
    entrypoint: ["/bin/sh"]
    command: ['-c', '/scripts/entrypoint.sh && resticprofile schedule --all && crond -f']
    restart: always

  jaeger:
    image: jaegertracing/all-in-one:latest
    volumes:
      - "./config/telemetry/jaeger-ui.json:/etc/jaeger/jaeger-ui.json"
    command: --query.ui-config /etc/jaeger/jaeger-ui.json
    environment:
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      # - "14250:14250"   # legacy jaeger-agent spans gRPC protobuf
      # - "14268:14268"   # legacy thrift_http receiver and sampling
      - "127.0.0.1:14269:14269"   # admin port: health check at / and metrics at /metrics
      # - "4317:4317"     # otlp collector over gRPC
      # - "4318:4318"     # otlp collector over http
      # - "6831:6831/udp" # legacy thrift in compact thrift protocol
      # - "6832:6832/udp" # legacy thrift in binary thrift protocol
      # - "9411:9411"     # zipkin v1 JSON or Thrift, v2 JSON or protobuf
      - "127.0.0.1:16686:16686"   # Jaeger UI
      # - "16685:16685"   # otlp-based protobuf, legacy protobuf read

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    volumes:
      - "./config/telemetry/otel-collector-config.yml:/etc/otelcol/otel-collector-config.yml"
    command:
      - "--config=/etc/otelcol/otel-collector-config.yml"
      - "--feature-gates=receiver.datadogreceiver.Enable128BitTraceID"
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      # - "4317:4317"   # OTLP gRPC receiver
      # - "4318:4318"   # OTLP HTTP receiver
      # - "8888:8888"   # Prometheus metrics
      # - "8889:8889"   # Prometheus exporter metrics
      # - "13133:13133" # health_check extension
      - "127.0.0.1:55679:55679" # ZPages for direct view
      # - "14268:14268" # default jaeger thrift_http receiver, but jaeger listens there
      # - "14278:14278" # alternate jaeger thrift_http receiver
      # - "8126:8126"   # datadog receiver for services instrumented with ddtrace
    depends_on:
      - jaeger
      - prometheus

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - "./config/telemetry/prometheus.yml:/etc/prometheus/prometheus.yml"
      # - "./data/prometheus:/prometheus"
    ports:
      # don't expose telemetry to public networks; other containers can connect directly, but expose the web interface on localhost
      - "127.0.0.1:9090:9090"
    command:
      # TODO? connect to metrics on new services
      - "--config.file=/etc/prometheus/prometheus.yml"
      # - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=200h"

